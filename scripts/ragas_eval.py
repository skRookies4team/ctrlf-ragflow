"""
RAGFlow + RAGAs ì—°ë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # ì™¸ë¶€ API (OpenAI) ì‚¬ìš©
    python scripts/ragas_eval.py --api-key YOUR_RAGFLOW_API_KEY --chat-id YOUR_CHAT_ID

    # ë¡œì»¬ LLM (Ollama) ì‚¬ìš© - íì‡„ë§ìš©
    python scripts/ragas_eval.py --api-key YOUR_API_KEY --chat-id YOUR_CHAT_ID --local-llm --ollama-url http://localhost:11434

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ íŒŒì¼ ì‚¬ìš©
    python scripts/ragas_eval.py --api-key YOUR_API_KEY --chat-id YOUR_CHAT_ID --questions questions.json

ì„¤ì¹˜:
    pip install ragas datasets langchain-community requests
"""

import argparse
import json
import requests
from typing import Optional
from datetime import datetime


def install_dependencies():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì•ˆë‚´"""
    print("""
í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:
    pip install ragas datasets langchain-community requests

íì‡„ë§ì—ì„œ Ollama ì‚¬ìš©ì‹œ:
    pip install langchain-ollama
""")


try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )
    from datasets import Dataset
except ImportError:
    install_dependencies()
    exit(1)


class RAGFlowClient:
    """RAGFlow API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def chat(self, chat_id: str, question: str, session_id: Optional[str] = None) -> dict:
        """RAGFlow ì±„íŒ… API í˜¸ì¶œ"""
        url = f"{self.base_url}/v1/chats/{chat_id}/completions"

        payload = {
            "question": question,
            "stream": False
        }
        if session_id:
            payload["session_id"] = session_id

        try:
            resp = requests.post(url, headers=self.headers, json=payload, timeout=120)
            resp.raise_for_status()
            return resp.json()
        except requests.exceptions.RequestException as e:
            print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_response(self, api_response: dict) -> dict:
        """API ì‘ë‹µì—ì„œ ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not api_response or "data" not in api_response:
            return {"answer": "", "contexts": []}

        data = api_response.get("data", {})
        answer = data.get("answer", "")

        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        contexts = []
        reference = data.get("reference", {})
        chunks = reference.get("chunks", [])

        for chunk in chunks:
            content = chunk.get("content", "") or chunk.get("content_with_weight", "")
            if content:
                contexts.append(content)

        return {
            "answer": answer,
            "contexts": contexts
        }


class RAGAsEvaluator:
    """RAGAs í‰ê°€ê¸°"""

    def __init__(self, use_local_llm: bool = False, ollama_url: str = "http://localhost:11434", model_name: str = "llama3.1:8b"):
        self.use_local_llm = use_local_llm
        self.ollama_url = ollama_url
        self.model_name = model_name
        self.llm = None
        self.embeddings = None

        if use_local_llm:
            self._setup_local_llm()

    def _setup_local_llm(self):
        """ë¡œì»¬ LLM ì„¤ì • (Ollama)"""
        try:
            from langchain_ollama import OllamaLLM, OllamaEmbeddings

            self.llm = OllamaLLM(
                model=self.model_name,
                base_url=self.ollama_url,
            )
            self.embeddings = OllamaEmbeddings(
                model=self.model_name,
                base_url=self.ollama_url,
            )
            print(f"âœ… ë¡œì»¬ LLM ì„¤ì • ì™„ë£Œ: {self.model_name} @ {self.ollama_url}")
        except ImportError:
            print("langchain-ollama ì„¤ì¹˜ í•„ìš”: pip install langchain-ollama")
            exit(1)
        except Exception as e:
            print(f"ë¡œì»¬ LLM ì„¤ì • ì‹¤íŒ¨: {e}")
            exit(1)

    def evaluate(self, questions: list, answers: list, contexts: list, ground_truths: list = None) -> dict:
        """RAGAs í‰ê°€ ì‹¤í–‰"""

        # ë°ì´í„°ì…‹ êµ¬ì„±
        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
        }

        # ì‚¬ìš©í•  ë©”íŠ¸ë¦­ ì„ íƒ
        metrics = [faithfulness, answer_relevancy, context_precision]

        # ground_truthê°€ ìˆìœ¼ë©´ context_recallë„ ì¸¡ì •
        if ground_truths and all(gt for gt in ground_truths):
            data["ground_truth"] = ground_truths
            metrics.append(context_recall)

        dataset = Dataset.from_dict(data)

        print(f"\nğŸ“Š í‰ê°€ ì‹œì‘ (ì§ˆë¬¸ {len(questions)}ê°œ)...")
        print(f"   ë©”íŠ¸ë¦­: {[m.name for m in metrics]}")

        # í‰ê°€ ì‹¤í–‰
        if self.use_local_llm and self.llm:
            result = evaluate(
                dataset,
                metrics=metrics,
                llm=self.llm,
                embeddings=self.embeddings,
            )
        else:
            # OpenAI API ì‚¬ìš© (OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”)
            result = evaluate(
                dataset,
                metrics=metrics,
            )

        return result


def load_questions(file_path: str) -> list:
    """ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ (JSON í˜•ì‹)"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    if isinstance(data, list):
        if isinstance(data[0], str):
            return [{"question": q} for q in data]
        return data
    elif isinstance(data, dict) and "questions" in data:
        return data["questions"]

    return data


def save_results(results: dict, output_path: str):
    """ê²°ê³¼ ì €ì¥"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="RAGFlow + RAGAs í‰ê°€ ìŠ¤í¬ë¦½íŠ¸")

    # í•„ìˆ˜ ì¸ì
    parser.add_argument("--api-key", required=True, help="RAGFlow API í‚¤")
    parser.add_argument("--chat-id", required=True, help="RAGFlow Chat ID")

    # ì„ íƒ ì¸ì
    parser.add_argument("--base-url", default="http://localhost:9380", help="RAGFlow API URL")
    parser.add_argument("--questions", help="ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ (JSON)")
    parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")

    # ë¡œì»¬ LLM ì˜µì…˜
    parser.add_argument("--local-llm", action="store_true", help="ë¡œì»¬ LLM ì‚¬ìš© (Ollama)")
    parser.add_argument("--ollama-url", default="http://localhost:11434", help="Ollama ì„œë²„ URL")
    parser.add_argument("--model", default="llama3.1:8b", help="Ollama ëª¨ë¸ëª…")

    args = parser.parse_args()

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    default_questions = [
        {"question": "RAGFlowì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"},
        {"question": "ë¬¸ì„œ ì²­í‚¹ì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?"},
        {"question": "ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?"},
    ]

    # ì§ˆë¬¸ ë¡œë“œ
    if args.questions:
        questions_data = load_questions(args.questions)
    else:
        print("âš ï¸  ì§ˆë¬¸ íŒŒì¼ ë¯¸ì§€ì •. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì‚¬ìš©.")
        questions_data = default_questions

    # RAGFlow í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = RAGFlowClient(args.base_url, args.api_key)

    # RAGAs í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = RAGAsEvaluator(
        use_local_llm=args.local_llm,
        ollama_url=args.ollama_url,
        model_name=args.model,
    )

    # ë°ì´í„° ìˆ˜ì§‘
    questions = []
    answers = []
    contexts = []
    ground_truths = []

    print("\nğŸ” RAGFlowì—ì„œ ì‘ë‹µ ìˆ˜ì§‘ ì¤‘...")

    for i, q_data in enumerate(questions_data):
        question = q_data["question"] if isinstance(q_data, dict) else q_data
        ground_truth = q_data.get("ground_truth", "") if isinstance(q_data, dict) else ""

        print(f"   [{i+1}/{len(questions_data)}] {question[:50]}...")

        # RAGFlow API í˜¸ì¶œ
        response = client.chat(args.chat_id, question)
        result = client.extract_response(response)

        if result["answer"]:
            questions.append(question)
            answers.append(result["answer"])
            contexts.append(result["contexts"])
            ground_truths.append(ground_truth)
        else:
            print(f"   âš ï¸  ì‘ë‹µ ì—†ìŒ: {question[:30]}...")

    if not questions:
        print("âŒ ìˆ˜ì§‘ëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # RAGAs í‰ê°€ ì‹¤í–‰
    try:
        scores = evaluator.evaluate(questions, answers, contexts, ground_truths)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š RAGAs í‰ê°€ ê²°ê³¼")
        print("="*60)

        for metric, score in scores.items():
            if isinstance(score, (int, float)):
                print(f"   {metric}: {score:.4f}")

        print("="*60)

        # ê²°ê³¼ ì €ì¥
        if args.output:
            output_path = args.output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"ragas_result_{timestamp}.json"

        save_data = {
            "timestamp": datetime.now().isoformat(),
            "config": {
                "base_url": args.base_url,
                "chat_id": args.chat_id,
                "local_llm": args.local_llm,
                "model": args.model if args.local_llm else "openai",
            },
            "scores": {k: float(v) if isinstance(v, (int, float)) else v for k, v in scores.items()},
            "details": [
                {
                    "question": q,
                    "answer": a[:200] + "..." if len(a) > 200 else a,
                    "context_count": len(c),
                }
                for q, a, c in zip(questions, answers, contexts)
            ]
        }

        save_results(save_data, output_path)

    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    main()
