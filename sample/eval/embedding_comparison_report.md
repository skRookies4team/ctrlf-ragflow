# 임베딩 모델 비교 테스트 보고서

**프로젝트명:** 기업 내부 정보보호 AI 어시스턴트 RAG 모듈 임베딩 비교 테스트
**작성일:** 2025-12-08
**작성자:** AI RAG 개발팀

---

## 1. 테스트 목적 및 범위

본 테스트는 사내 규정/교육 도메인 RAG 시스템에서 사용할 최종 임베딩 모델 선정을 위해 수행되었다. 사규, 복무, 개인정보보호, 성희롱예방, 직장 내 괴롭힘, 장애인식개선, 직무교육 등 7개 도메인에 걸쳐 6개 후보 임베딩 모델의 검색 품질을 비교 평가하였다.

**본 보고서는 임베딩 모델에 따른 검색(Retrieval) 품질 비교에 초점을 두었으며, LLM 생성 답변 품질(AnswerRelevancy, Faithfulness 등)은 후속 RAGAS 기반 평가에서 별도로 다룰 예정이다.**

---

## 2. 테스트 환경 및 공통 조건

| 항목 | 설정값 |
|------|--------|
| **LLM 모델** | Qwen2-7B-Instruct (vLLM 서버, 포트 1234) |
| **전처리/청킹 방식** | 문자 기준, chunk size 500, overlap 50 |
| **Vector DB / 인덱스** | 코사인 유사도 기반 직접 계산 (NumPy) |
| **문서 형식** | TXT (3개), PDF (19개) |
| **총 청크 수** | 2,895개 |
| **임베딩 서버** | vLLM (localhost:1237), OpenAI 호환 API |

---

## 3. 평가 데이터셋(Q세트) 설명

| 항목 | 내용 |
|------|------|
| **총 문항 수** | 130문항 (헤더 제외) |
| **도메인별 분포** | 사규/복무/인사 30개, 개인정보보호 20개, 성희롱예방교육 20개, 직장내괴롭힘예방교육 20개, 장애인식교육 20개, 직무(부서별)교육 20개 |
| **난이도 분포** | Easy 17개(13%), Medium 92개(71%), Hard 21개(16%) |
| **사용자 롤 분포** | 일반직원 119개(92%), 관리자 11개(8%) |

본 Q세트는 기업 내부 규정 및 법정 의무교육 도메인에 특화되어 있으며, 일반 직원이 실무에서 자주 묻는 질문 유형을 중심으로 구성되었다. 따라서 본 테스트 결과는 해당 도메인 범위 내에서 유효하며, 기술/개발/영업 등 타 직무 도메인에 대해서는 별도 평가가 필요하다.

---

## 4. 평가 지표 정의

본 테스트에서는 검색(Retrieval) 품질을 측정하기 위해 다음 지표를 사용하였다.

| 지표 | 설명 | 중요도 |
|------|------|--------|
| **Precision@K** | 상위 K개 검색 결과 중 관련 문서의 비율 (0~1). K=1일 때 가장 엄격한 정확도 측정. | 정확도 관점 |
| **Recall@K** | 상위 K개 안에 정답 문서가 포함되었는지 여부 (0 또는 1). 검색 누락을 측정. | 재현율 관점 |
| **MRR (Mean Reciprocal Rank)** | 정답 문서가 검색 결과 몇 번째에 등장하는지의 역수 평균 (0~1). 1에 가까울수록 정답이 상위에 위치. | 순위 품질 관점 |
| **NDCG@5** | 상위 5개 결과의 순위 가중 품질 점수 (0~1). 정답이 상위권에 있을수록 높은 점수. | 종합 순위 품질 |
| **Hit Rate@K** | 상위 K개 안에 정답이 1개 이상 포함된 비율 (0~1). 실사용 관점의 성공률. | 사용자 경험 관점 |
| **Latency** | 질문 1건 임베딩 생성 평균 시간 (ms). 낮을수록 응답 속도 우수. | 성능 관점 |

---

## 5. 실험 결과 요약

### 5.1 임베딩별 비교 표

| 실험_ID | 임베딩 모델 | 차원 | P@1 | P@3 | P@5 | R@5 | MRR | NDCG@5 | Hit@5 | Latency(ms) |
|---------|-------------|------|-----|-----|-----|-----|-----|--------|-------|-------------|
| EXP-E01 | jhgan/ko-sroberta-multitask | 768 | **0.731** | **0.705** | **0.700** | **0.777** | **0.749** | **0.750** | **0.777** | 87.35 |
| EXP-E02 | BAAI/bge-m3 | 1024 | 0.715 | 0.659 | 0.642 | 0.777 | 0.739 | 0.738 | 0.777 | 99.49 |
| EXP-E03 | nlpai-lab/KURE-v1 | 1024 | 0.708 | 0.674 | 0.655 | 0.762 | 0.736 | 0.732 | 0.762 | 66.95 |
| EXP-E04 | BM-K/KoSimCSE-roberta-multitask | 768 | 0.662 | 0.621 | 0.611 | 0.715 | 0.691 | 0.680 | 0.715 | **30.85** |
| EXP-E05 | dragonkue/multilingual-e5-small-ko | 384 | 0.654 | 0.669 | 0.657 | 0.738 | 0.696 | 0.704 | 0.738 | 57.91 |
| EXP-E06 | intfloat/multilingual-e5-large-instruct | 1024 | 0.308 | 0.387 | 0.412 | 0.685 | 0.480 | 0.531 | 0.685 | 75.24 |

### 5.2 주요 관찰

1. **jhgan/ko-sroberta-multitask**가 P@1(0.731), MRR(0.749), NDCG@5(0.750) 전 지표에서 1위를 기록하였다.
2. **BAAI/bge-m3**와 **nlpai-lab/KURE-v1**이 2, 3위로 근소한 차이를 보였다.
3. **intfloat/multilingual-e5-large-instruct**는 P@1이 0.308로 현저히 낮았다. 이는 instruct 모델 특성상 "query: ", "passage: " 등 특수 프롬프트 형식이 필요할 수 있으나, 본 테스트에서는 일반 텍스트로 입력하여 성능 저하가 발생한 것으로 추정된다.
4. **BM-K/KoSimCSE**는 Latency 30.85ms로 가장 빠르나, 정확도 지표는 중하위권이다.

---

## 6. 결과 해석 및 임베딩 모델 선정 근거

### 6.1 모델별 분석

| 모델 | 강점 | 약점 | 적합성 |
|------|------|------|--------|
| **jhgan/ko-sroberta-multitask** | 전 지표 1위, 한국어 특화, 768차원으로 적절한 리소스 | Latency 87ms로 중간 수준 | **1순위 추천** |
| **BAAI/bge-m3** | Hit@5 공동 1위, 다국어 지원 | 1024차원으로 리소스 사용량 높음, Latency 99ms | 2순위 백업 |
| **nlpai-lab/KURE-v1** | 한국어 특화, Latency 67ms | 정확도 3위 | 속도-정확도 균형 시 고려 |
| **BM-K/KoSimCSE** | Latency 31ms로 가장 빠름 | 정확도 하위권 | 속도 최우선 시 고려 |
| **dragonkue/e5-small-ko** | 384차원으로 가장 가벼움 | 정확도 중하위 | 리소스 제약 심할 때 고려 |
| **intfloat/e5-large-instruct** | - | P@1 0.308으로 부적합 | 미채택 |

### 6.2 선정 결론

- **1차 운영 후보:** `jhgan/ko-sroberta-multitask`
  - 근거: MRR 0.749, NDCG@5 0.750으로 전 지표 1위. 768차원으로 리소스 효율적. 한국어 문장 임베딩에 특화된 모델.

- **백업 후보:** `BAAI/bge-m3`
  - 근거: Hit@5 공동 1위(0.777). 다국어 지원으로 향후 영문 문서 확장 시 유리. 단, 1024차원으로 메모리 사용량 증가.

**사내 규정/교육 도메인 RAG의 1차 목표를 P@1 ≥ 0.70, MRR ≥ 0.73 수준으로 설정하였으며, jhgan/ko-sroberta-multitask는 이 기준을 만족하는 유일한 모델이었다.**

---

## 7. 제한사항 및 주의사항

1. **Q세트 규모:** 130문항으로, 통계적 유의성을 높이려면 300문항 이상 확보가 권장된다.

2. **도메인 범위:** 사규/교육 도메인에 한정되어 있으므로, 기술/영업/마케팅 등 타 도메인 적용 시 재평가 필요.

3. **청킹 방식:** chunk size 500, overlap 50의 단순 문자 기반 청킹을 사용하였으며, 문단/의미 단위 청킹 적용 시 결과가 달라질 수 있다.

4. **e5-large-instruct 모델:** 본 테스트에서 특수 프롬프트 형식 미적용으로 성능이 과소평가되었을 가능성이 있다. 단, 본 프로젝트의 리소스·복잡도 관점에서 instruct 계열 임베딩은 우선순위를 낮게 두고, 추후 필요 시 별도 실험으로 분리하는 것이 타당하다고 판단하였다.

5. **LLM 기반 평가 미수행:** 본 테스트는 검색(Retrieval) 단계만 평가하였으며, LLM 답변 품질(AnswerRelevancy, Faithfulness 등)은 별도 평가가 필요하다.

---

## 8. 결론 및 다음 액션

### 8.1 최종 추천

| 순위 | 모델 | 용도 |
|------|------|------|
| 1순위 | **jhgan/ko-sroberta-multitask** | 프로덕션 운영 |
| 2순위 | BAAI/bge-m3 | 백업/다국어 확장 시 |

### 8.2 단기 액션 (1-2주)

- [ ] jhgan/ko-sroberta-multitask로 프로덕션 인덱스 구성
- [ ] RAGFlow 설정에 선정 모델 반영 (`conf/service_conf.yaml`)
- [ ] 청킹 방식 개선 (문단/의미 단위) 후 2차 평가 수행

### 8.3 중기 액션 (1개월)

- [ ] Q세트 300문항 이상으로 확대
- [ ] LLM 모델 비교 테스트 (Qwen2-7B vs Llama3-8B vs Gemma3-12B)
- [ ] 선정된 임베딩 + LLM 조합에 대해 RAGAS 기반 LLM 답변 품질 평가 수행 (AnswerRelevancy, Faithfulness, ContextRecall, ContextPrecision)
- [ ] Reranker 모델 도입 검토 (bge-reranker-v2 등)

---

## 부록 A: 실험 환경 상세

```
서버: localhost (vLLM)
포트: 1237 (Embedding)
테스트 일시: 2025-12-08
문서 수: TXT 3개 + PDF 19개 = 22개
총 청크 수: 2,895개
질문 수: 130개
평가 스크립트: sample/eval/embedding_eval.py
```

---

## 부록 B: Q세트 예시

실제 테스트에 사용된 질문 예시:

| 도메인 | 질문 예시 | 출처 문서 |
|--------|-----------|-----------|
| **사규/복무** | "상사가 지시한 내용이 사규와 충돌하는 것 같을 때, 직원은 어떻게 행동하는 것이 바람직한가요?" | DOC-사규관리규정 |
| **성희롱예방** | "회식 자리에서 술을 강요하거나 노래방에서 듀엣을 강제하는 행위가 성희롱에 해당할 수 있나요?" | DOC-성희롱규정 |
| **개인정보보호** | "업무 중 알게 된 고객의 이름·전화번호를 개인 메신저나 개인 클라우드에 저장해도 되나요?" | DOC-정보보안정책 |
| **직장내괴롭힘** | "업무 지시를 가장하여 특정 직원에게만 반복적으로 허드렛일을 시키는 것이 괴롭힘인가요?" | DOC-직장내괴롭힘규정 |
| **장애인식교육** | "장애가 있는 동료를 도울 때 먼저 어떻게 접근해야 하나요?" | DOC-장애인식교육 |
