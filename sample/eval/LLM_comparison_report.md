# LLM 비교 테스트 결과 보고서

**프로젝트**: CTRL-F (기업 내부 정보보호 AI 어시스턴트)
**작성일**: 2025-12-08
**테스트 유형**: 순수 LLM 답변 품질 비교 (Pure LLM, RAG 미적용)

---

## 1. 테스트 목적 및 범위

### 목적
사내 RAG 시스템에서 사용할 최종 LLM 후보를 선정하기 위해, 3개 LLM의 **답변 품질·응답 속도·답변 길이**를 비교한다.

### 범위
- **테스트 방식**: 순수 LLM 모드 (RAG 검색 없이 모델 자체의 한국어 답변 능력 평가)
- **평가 관점**: 한국어 답변 품질, 응답 속도, 답변 완성도
- **후속 평가 예정**: RAG 파이프라인 적용 후 RAGAS 기반 정량 평가

### 비교 대상 LLM

| 모델명 | 파라미터 | 비고 |
|--------|----------|------|
| meta-llama/Meta-Llama-3-8B-Instruct | 8B | Meta 공식 Instruct 모델 |
| Qwen/Qwen2-7B-Instruct | 7B | Alibaba Qwen2 시리즈 |
| Qwen/Qwen2.5-7B-Instruct | 7B | Qwen2 개선 버전 |

> **제외 모델**: google/gemma-3-12b-it (12B) - 파라미터 크기로 인해 파인튜닝 시 자원 효율성 문제 예상

---

## 2. 테스트 환경 및 공통 조건

### 고정 조건 (LLM만 변경)

| 항목 | 설정값 |
|------|--------|
| **서버 환경** | 동일 GPU 서버 (환경변수: VLLM_HOST, VLLM_PORT) |
| **추론 엔진** | vLLM (OpenAI 호환 API) |
| **Temperature** | 0.1 (일관성 높은 답변 유도) |
| **Max Tokens** | 1024 |
| **프롬프트 언어** | 한국어 시스템 프롬프트 + 한국어 질문 |

### 프롬프트 템플릿

```
시스템: 당신은 회사 내규 및 정책에 대해 답변하는 AI 어시스턴트입니다.
       질문에 대해 일반적인 기업 관행과 한국 노동법을 기반으로 답변해주세요.
       반드시 한국어로 답변해주세요.

사용자: {질문}
```

### 모델별 프롬프트 형식
- **Llama-3**: `<|begin_of_text|>...<|eot_id|>` 형식
- **Qwen2/2.5**: `<|im_start|>...<|im_end|>` ChatML 형식

---

## 3. 평가 데이터셋(Q세트) 설명

### 데이터셋 개요

| 항목 | 값 |
|------|-----|
| 총 문항 수 | **130개** |
| 출처 | 기업 내부 규정 기반 자체 제작 |
| 형식 | 질문 + 모범답안 + 출처문서ID |

### 도메인별 분포

| 도메인 | 문항 수 | 비율 |
|--------|---------|------|
| 사규/복무/인사 | 30개 | 23.1% |
| 개인정보보호 | 20개 | 15.4% |
| 성희롱예방교육 | 20개 | 15.4% |
| 직장내괴롭힘방지교육 | 20개 | 15.4% |
| 장애인식개선교육 | 20개 | 15.4% |
| 직무(부서별)교육 | 20개 | 15.4% |

### 사용자 롤 분포

| 롤 | 문항 수 | 비율 |
|----|---------|------|
| 일반직원 | 119개 | 91.5% |
| 관리자 | 11개 | 8.5% |

### 난이도 분포

| 난이도 | 문항 수 | 비율 |
|--------|---------|------|
| Easy | 17개 | 13.1% |
| Medium | 92개 | 70.8% |
| Hard | 21개 | 16.2% |

### 평가 유효 범위
본 Q세트는 기업 내부 규정(사규, 복무, 개인정보보호, 법정교육) 관련 질문으로 구성되어 있으며, 대부분 **규정 조회형/절차 안내형** 질문이다. 따라서 이번 평가 결과는 "내부 규정 기반 Q&A 시나리오"에서의 LLM 성능을 대표한다.

---

## 4. 평가 지표 정의

### 이번 테스트에서 측정한 지표

| 지표 | 설명 | 중요도 |
|------|------|--------|
| **평균 응답 시간** | 질문당 답변 생성 소요 시간 (초) | 사용자 경험, 서비스 응답성 |
| **평균 답변 길이** | 생성된 답변의 평균 문자 수 | 답변 상세도, 토큰 비용 |
| **총 소요 시간** | 130개 질문 전체 처리 시간 | 처리량(throughput) |

### 후속 평가 예정 지표 (RAGAS 기반)

| 지표 | 설명 |
|------|------|
| Answer Relevancy | 답변이 질문에 얼마나 관련성 있는지 (0~1) |
| Faithfulness | 답변이 제공된 context에 충실한지 (0~1) |
| Context Utilization | 검색된 문서를 얼마나 활용했는지 (0~1) |

### 휴먼 평가 예정 지표

| 지표 | 설명 |
|------|------|
| 정확성 | 규정/법령과 일치 여부 |
| 충실성 | 필요한 정보 누락 여부 |
| 명확성 | 일반 직원이 이해하기 쉬운지 |
| 안전성 | PII 노출, 정책 위반 발언 여부 |

---

## 5. 실험 설계 및 프로토콜

### 실험 방식

1. **Q세트 전체 사용**: 130개 문항 전체를 각 LLM에 순차 질의
2. **1회 생성**: 동일 질문에 대해 각 LLM 1회 답변 생성 (temperature=0.1로 재현성 확보)
3. **순수 LLM 모드**: RAG 검색 없이 모델 자체 지식으로 답변 생성
4. **결과 저장**: JSON 형식으로 질문/답변/응답시간 기록

### 평가 절차

```
1단계: vLLM 서버에 모델 로드
2단계: 130개 질문 순차 질의 및 답변 수집
3단계: 응답 시간/답변 길이 통계 산출
4단계: 모델별 결과 JSON 파일 저장
```

### 무작위성 통제
- Temperature 0.1로 고정하여 답변 일관성 확보
- 동일 프롬프트 템플릿 사용
- 동일 서버/포트에서 순차 실행

---

## 6. 실험 결과 요약

### 6.1 정량 지표 비교

| 모델 | 평균 응답 시간 | 총 소요 시간 | 평균 답변 길이 | 최소/최대 응답 |
|------|---------------|-------------|---------------|---------------|
| **Llama-3-8B** | 3.73s | 8.1분 | **1,343자** | 1.95s / 9.06s |
| **Qwen2-7B** | **3.76s** | **8.1분** | 726자 | 1.77s / 6.64s |
| **Qwen2.5-7B** | 4.02s | 8.7분 | 767자 | 1.17s / 8.87s |

### 6.2 결과 해석

#### 응답 속도
- **Llama-3-8B**와 **Qwen2-7B**가 동일하게 8.1분으로 가장 빠름
- Qwen2.5-7B는 약 7% 느림 (8.7분)
- 세 모델 모두 실시간 서비스에 적합한 수준 (평균 4초 이내)

#### 답변 길이
- **Llama-3-8B**: 가장 장문 (평균 1,343자) - 상세하지만 장황할 가능성
- **Qwen2-7B/2.5-7B**: 간결함 (평균 720~770자) - 핵심 위주 답변

#### 응답 시간 안정성
- **Qwen2-7B**: 가장 안정적 (최대 6.64s)
- **Llama-3-8B**: 변동폭 큼 (최대 9.06s)
- **Qwen2.5-7B**: 변동폭 가장 큼 (최소 1.17s ~ 최대 8.87s)

### 6.3 샘플 답변 비교

**질문**: "우리 회사의 기본 근무시간과 휴게시간은 어떻게 되나요?"

| 모델 | 답변 특징 |
|------|----------|
| **Llama-3-8B** | 노동법 기준 상세 설명, 추가 휴게시간 예시까지 언급 (415자) |
| **Qwen2-7B** | 주 52시간제, 휴게시간 법적 기준 설명, HR 확인 권고 (500자) |
| **Qwen2.5-7B** | 가장 간결, 핵심만 전달, HR 문의 안내 (355자) |

---

## 7. 보안·안전 관점 분석

### 7.1 PII/민감정보 관점
- 이번 테스트는 순수 LLM 모드로, 실제 고객/직원 개인정보가 입력되지 않음
- RAG 적용 시 PII 마스킹 파이프라인 적용 필요

### 7.2 정책 준수 관점
- 세 모델 모두 한국 노동법 기반의 일반적 답변 제공
- 회사별 특수 규정은 "회사 규정 확인" 또는 "HR 문의" 권고로 대응
- **hallucination 위험**: 순수 LLM 모드에서는 존재하지 않는 규정을 생성할 가능성 있음 → RAG 적용으로 완화 필요

### 7.3 안전성 관련 관찰
- 세 모델 모두 부적절하거나 위험한 답변은 관찰되지 않음
- 모든 모델이 "정확한 정보는 회사 규정/담당 부서 확인" 권고 포함

---

## 8. 결과 해석 및 최종 LLM 선정 근거

### 8.1 관점별 우수 모델

| 평가 관점 | 추천 모델 | 근거 |
|-----------|----------|------|
| **속도 효율** | Qwen2-7B | 가장 빠른 응답, 안정적 latency |
| **답변 상세도** | Llama-3-8B | 가장 긴 답변, 상세한 설명 |
| **간결성** | Qwen2.5-7B | 핵심 위주 간결한 답변 |
| **비용 효율** | Qwen2-7B / Qwen2.5-7B | 짧은 답변 = 적은 토큰 비용 |
| **파인튜닝 적합성** | Qwen2-7B / Qwen2.5-7B | 7B 파라미터로 자원 효율적 |

### 8.2 종합 평가

| 순위 | 모델 | 총점 | 강점 | 약점 |
|------|------|------|------|------|
| 1 | **Qwen2.5-7B** | ⭐⭐⭐⭐ | 간결한 답변, 한국어 품질 양호, 최신 모델 | 약간 느린 속도 |
| 2 | **Qwen2-7B** | ⭐⭐⭐⭐ | 가장 빠름, 안정적, 간결함 | Qwen2.5 대비 구버전 |
| 3 | **Llama-3-8B** | ⭐⭐⭐ | 상세한 답변 | 장황함, 토큰 비용 높음 |

### 8.3 최종 추천

**1순위: Qwen2.5-7B-Instruct**
- 최신 Qwen 시리즈로 한국어 성능 개선
- 간결하면서도 핵심을 담은 답변 스타일
- 7B 파라미터로 파인튜닝 자원 효율적

**2순위: Qwen2-7B-Instruct**
- 속도와 안정성 최우수
- Qwen2.5와 유사한 답변 품질
- 검증된 안정성

---

## 9. 제한사항 및 주의사항

### 9.1 테스트 제한사항
1. **순수 LLM 테스트만 수행**: RAG 파이프라인 적용 시 결과가 달라질 수 있음
2. **자동 지표 미측정**: RAGAS 기반 정량 평가(Faithfulness, Relevancy 등) 미수행
3. **휴먼 평가 미수행**: 정확성, 안전성에 대한 전문가 검토 필요
4. **단일 실행**: 각 질문당 1회 생성으로, 답변 일관성 검증 부족

### 9.2 해석 주의사항
- 답변 길이가 길다고 품질이 높은 것은 아님 (장황함 vs 상세함)
- 순수 LLM 답변은 hallucination 위험이 있으므로 RAG 적용 필수
- 파인튜닝 후 성능이 크게 달라질 수 있음

### 9.3 후속 검증 필요 항목
- [ ] RAG 파이프라인 적용 후 동일 테스트 수행
- [ ] RAGAS 기반 자동 평가 지표 측정
- [ ] 휴먼 평가 (정확성, 안전성, 명확성)
- [ ] 도메인별/난이도별 세부 분석

---

## 10. 결론 및 다음 액션

### 10.1 결론
3개 LLM 중 **Qwen2.5-7B-Instruct**를 1차 후보로 선정한다.
- 한국어 답변 품질 양호
- 간결하고 핵심적인 답변 스타일
- 7B 파라미터로 파인튜닝 효율적
- 속도 차이는 실사용에 큰 영향 없음 (0.3초 차이)

### 10.2 다음 액션

#### 1단계: RAG 파이프라인 통합 테스트
- [ ] RAGFlow에 Qwen2.5-7B 연동
- [ ] 동일 Q세트로 RAG 모드 테스트
- [ ] RAGAS 평가 수행 (Faithfulness, Answer Relevancy, Context Utilization)

#### 2단계: 휴먼 평가 및 최종 선정
- [ ] 전문가 2~3인 블라인드 평가
- [ ] 정확성/안전성 집중 검토
- [ ] 최종 모델 확정

#### 3단계: 파인튜닝 준비
- [ ] 도메인 특화 학습 데이터 구축
- [ ] LoRA/QLoRA 파인튜닝 실험
- [ ] 파인튜닝 전후 성능 비교

### 10.3 KPI 목표

| 지표 | 현재 (순수 LLM) | 목표 (RAG + 파인튜닝) |
|------|----------------|---------------------|
| Answer Relevancy | 미측정 | ≥ 0.85 |
| Faithfulness | 미측정 | ≥ 0.90 |
| 평균 응답 시간 | 4.0초 | ≤ 5.0초 |
| 휴먼 정확성 | 미측정 | ≥ 4.0/5.0 |

---

**작성**: LLM 비교 테스트 자동화 스크립트
**검토 필요**: 멘토/CTO 승인 후 다음 단계 진행
