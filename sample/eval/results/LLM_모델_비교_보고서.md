# LLM 모델 비교 평가 보고서

**프로젝트**: CTRL-F (기업 내부 정보보호 AI 어시스턴트)
**평가일시**: 2025-12-10 (130문항 전체 평가 완료)
**평가자**: AI 평가 시스템
**평가 단계**: 1단계 (응답 시간·답변 길이) + 2단계 (LLM-as-Judge 품질 평가)

---

## 1. 평가 개요

### 1.1 평가 목적
기업 내부 문서 기반 RAG 시스템에 적합한 LLM 모델 선정을 위한 성능 비교

> **참고**: 이번 보고서는 **1단계(응답 시간·답변 길이)와 2단계(LLM-as-Judge 품질 평가)** 결과를 포함한다.
> 휴먼 평가 및 안전성 평가는 향후 3단계에서 별도로 수행할 예정이다.

> **전체 평가 완료**: RAGAS 스타일 품질 평가가 **모델당 전체 130문항**에 대해 완료되었다.
> 이 결과는 예비 분석이 아닌 **전체 데이터셋 기반의 최종 평가 결과**로 해석할 수 있다.

### 1.2 평가 환경
임베딩 모델과 검색 파이프라인은 이전 임베딩 비교 실험에서 선정한 설정을 그대로 사용하고,
이번 평가는 LLM 모델만 변경하여 응답 특성을 비교하였다.
- **서버**: 환경변수 VLLM_HOST
- **임베딩 모델**: jhgan/ko-sroberta-multitask (환경변수 VLLM_PORT 참조)
- **LLM 모델 서버**: vLLM
- **문서 청킹**: 300자 단위, 30자 오버랩
- **검색 방식**: 코사인 유사도 기반 Top-5 검색

### 1.3 평가 대상 모델
| 모델명 | 파라미터 | 특징 |
|--------|----------|------|
| Qwen/Qwen2-7B-Instruct | 7B | 중국 Alibaba, 다국어 지원 |
| Qwen/Qwen2.5-7B-Instruct | 7B | Qwen2 후속, 성능 개선 |
| meta-llama/Meta-Llama-3-8B-Instruct | 8B | Meta AI, 영어 기반 |

### 1.4 테스트 데이터셋
- **총 질문 수**: 130개
- **도메인 분류**:
  - 사규/복무/인사: 30개
  - 개인정보보호: 20개
  - 성희롱예방교육: 20개
  - 직장내괴롭힘예방교육: 20개
  - 장애인식교육: 20개
  - 직무(부서별)교육: 20개

### 1.5 평가 지표 정의

- **평균 응답시간**
  - 질문 1건에 대해 LLM이 최종 답변을 반환하기까지 걸린 평균 시간(sec)
  - 실시간 챗봇 UX와 서버 부하 측면에서 중요한 지표

- **평균 답변 길이**
  - LLM이 생성한 최종 답변의 평균 글자 수(한글 기준)
  - 길이가 길다고 해서 항상 더 좋은 것은 아니며,
    동일 도메인에서 "필요한 내용을 충분히 다루고 있는지"를 간접적으로 보는 참고 지표

- **성공률**
  - LLM이 모든 질문에 대해 **형식 오류 없이 정상적인 한국어 답변을 반환한 비율**
  - 이번 실험에서는 세 모델 모두 100%로, "응답 생성 실패가 없었다"는 의미이며,
    내용의 정확성/품질을 의미하지는 않음

- **Faithfulness (충실도)**
  - 답변이 검색된 컨텍스트에 충실하게 기반하고 있는지 평가 (0.0 ~ 1.0)
  - 컨텍스트에 없는 정보를 지어내지 않았는지 확인

- **Relevancy (관련성)**
  - 답변이 질문에 직접적으로 관련 있고 적절하게 답하고 있는지 평가 (0.0 ~ 1.0)

- **Correctness (정확도)**
  - 답변이 모범답안(Ground Truth)과 얼마나 일치하는지 평가 (0.0 ~ 1.0)
  - 핵심 정보 포함 여부와 정보의 정확성 확인

> **용어 정리**: 위 세 지표(Faithfulness, Relevancy, Correctness)는 **RAGAS 라이브러리**에서 사용하는 개념을 참고하였다.
> 단, 이번 평가에서는 공식 RAGAS 패키지를 직접 사용하지 않고, 동일한 개념을 **Qwen2.5-7B를 Judge LLM으로 사용하는 LLM-as-Judge 방식**으로 구현하여 산출하였다.

### 1.6 평가 방법

- **RAG 조건**
  - 모든 LLM은 동일한 RAG 결과(Top-5 컨텍스트)를 시스템 프롬프트와 함께 입력받도록 구성
  - 검색 파이프라인(임베딩, 청킹, Top-5 검색)은 동일한 설정을 그대로 사용

- **프롬프트 설정**
  - temperature = 0.1, max_tokens = 1024
  - 동일한 시스템 프롬프트(역할: 회사 내규 및 정책 답변 AI, 한국어 답변)를 사용하여 모델 간 편차를 최소화
  - 모델별 프롬프트 템플릿 적용 (Qwen: ChatML 형식, Llama3: Instruct 형식)

- **실행 방식**
  - 각 질문에 대해 LLM별로 1회 생성
  - 모든 질문은 같은 환경(동일 서버, 동일 설정)에서 순차 실행

- **품질 평가 방식 (LLM-as-Judge)**
  - Judge LLM: Qwen2.5-7B-Instruct
  - 샘플 크기: **모델당 전체 130문항**
  - 평가 항목: Faithfulness, Relevancy, Correctness
  - 주의: 동일 모델 계열(Qwen2.5)이 평가를 수행하므로 Qwen 모델에 약간의 편향 가능성 있음

---

## 2. 전체 성능 비교

### 2.1 종합 결과
요약하면, 응답 속도는 Llama3-8B가 가장 빠르고, 답변 길이는 Qwen2가 가장 길며,
속도·길이 균형 측면에서는 Qwen2.5-7B가 중간값에 위치한다.

#### 1단계: 응답 성능 지표

| 모델 | 평균 응답시간 | 평균 답변길이 | 성공률 |
|------|--------------|--------------|--------|
| **Qwen2-7B** | 3.82s | 522자 | 100% |
| **Qwen2.5-7B** | 2.94s | 469자 | 100% |
| **Llama3-8B** | 2.44s | 361자 | 100% |

#### 2단계: 품질 평가 지표 (LLM-as-Judge, 130문항 전체)

| 모델 | Faithfulness | Relevancy | Correctness | 평균 |
|------|-------------|-----------|-------------|------|
| **Qwen2-7B** | 0.653 | 0.938 | **0.666** | 0.752 |
| **Qwen2.5-7B** | **0.705** | **0.947** | 0.655 | **0.769** |
| **Llama3-8B** | 0.609 | 0.796 | 0.525 | 0.643 |

> **해석**: 130문항 전체 평가 결과, **Qwen2.5-7B가 품질 평가에서 가장 높은 평균 점수(0.769)를 기록**했다.
> 특히 Faithfulness(0.705)와 Relevancy(0.947)에서 모든 모델 중 가장 우수한 성능을 보였다.
> Qwen2-7B도 Correctness(0.666)에서 최고점을 기록하며 근소한 차이로 2위를 차지했다.
> Llama3-8B는 모든 지표에서 가장 낮은 점수를 받아, 한국어 문서 기반 RAG 태스크에서는 한계가 있는 것으로 확인되었다.

> **KPI 달성 현황**: 사내 RAG 시스템의 1차 품질 목표선인 **Faithfulness ≥ 0.7, Relevancy ≥ 0.8**을 기준으로:
> - **Qwen2.5-7B**: Faithfulness **0.705** ✅, Relevancy **0.947** ✅ → **목표 달성**
> - **Qwen2-7B**: Faithfulness 0.653 ❌, Relevancy **0.938** ✅ → 부분 달성
> - **Llama3-8B**: Faithfulness 0.609 ❌, Relevancy 0.796 ❌ → 미달성

### 2.2 성능 분석

#### 응답 속도
```
Llama3-8B    ████████████████████████░░░░░░  2.44s (가장 빠름)
Qwen2.5-7B   ██████████████████████████████  2.94s
Qwen2-7B     █████████████████████████████████████  3.82s (가장 느림)
```

#### 답변 길이
```
Qwen2-7B     █████████████████████████████████████  522자 (가장 상세)
Qwen2.5-7B   █████████████████████████████████░░░  469자
Llama3-8B    ██████████████████████████░░░░░░░░░░  361자 (가장 간결)
```

#### 품질 지표 (LLM-as-Judge, 130문항)
```
평균 품질 점수:
Qwen2.5-7B   ██████████████████████████████████████  0.769 (가장 높음)
Qwen2-7B     █████████████████████████████████████░  0.752
Llama3-8B    █████████████████████████████░░░░░░░░░  0.643 (가장 낮음)
```

---

## 3. 도메인별 상세 분석

### 3.1 사규/복무/인사 (30문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 4.67s | 412자 |
| Qwen2.5-7B | 2.18s | 373자 |
| Llama3-8B | 1.84s | 251자 |

**분석**: Llama3가 가장 빠르지만 답변이 짧음. 복잡한 인사 규정 설명에는 Qwen2가 유리.

### 3.2 개인정보보호 (20문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 3.78s | 564자 |
| Qwen2.5-7B | 3.42s | 485자 |
| Llama3-8B | 2.44s | 373자 |

**분석**: 개인정보 관련 질문에서 Qwen2가 가장 상세한 답변 제공.

### 3.3 성희롱예방교육 (20문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 2.90s | 417자 |
| Qwen2.5-7B | 2.20s | 380자 |
| Llama3-8B | 2.14s | 361자 |

**분석**: 모든 모델이 비슷한 성능. 응답 시간 차이 크지 않음.

### 3.4 직장내괴롭힘예방교육 (20문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 2.85s | 471자 |
| Qwen2.5-7B | 2.83s | 419자 |
| Llama3-8B | 2.77s | 351자 |

**분석**: 응답 시간은 유사하나 Qwen2가 더 상세한 설명 제공.

### 3.5 장애인식교육 (20문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 3.91s | 635자 |
| Qwen2.5-7B | 3.14s | 539자 |
| Llama3-8B | 2.78s | 473자 |

**분석**: Qwen2가 가장 상세한 답변 생성. 장애 관련 민감한 주제에서 충분한 설명 제공.

### 3.6 직무(부서별)교육 (20문항)

| 모델 | 평균 응답시간 | 평균 답변길이 |
|------|--------------|--------------|
| Qwen2-7B | 4.39s | 688자 |
| Qwen2.5-7B | 4.23s | 666자 |
| Llama3-8B | 2.94s | 411자 |

**분석**: 직무 관련 복잡한 질문에서 Qwen 계열이 더 상세한 답변 제공.

---

## 4. 모델별 특성 요약

### 4.1 Qwen/Qwen2-7B-Instruct
- **장점**: 가장 상세하고 포괄적인 답변, Correctness 최고점 (0.666)
- **단점**: 응답 시간이 가장 느림 (3.82s), Faithfulness 목표 미달
- **품질 지표**: Faithfulness 0.653 | Relevancy **0.938** | Correctness **0.666**
- **품질 평균**: 0.752 (2위)
- **적합한 용도**: 정확성이 중요한 법률/규정 관련 질의응답

### 4.2 Qwen/Qwen2.5-7B-Instruct
- **장점**: 속도와 품질의 균형, **품질 평가 최고점 (0.769)**, KPI 목표 달성
- **단점**: 응답 시간이 중간 (2.94s)
- **품질 지표**: Faithfulness **0.705** ✅ | Relevancy **0.947** ✅ | Correctness 0.655
- **품질 평균**: **0.769** (1위)
- **적합한 용도**: **기업 내부 RAG 시스템의 메인 모델로 권장**

### 4.3 meta-llama/Meta-Llama-3-8B-Instruct
- **장점**: 가장 빠른 응답 속도 (Qwen2 대비 36% 빠름)
- **단점**: 답변이 가장 짧고, **품질 평가 최저점 (0.643)**, KPI 전 항목 미달
- **품질 지표**: Faithfulness 0.609 | Relevancy 0.796 | Correctness 0.525
- **품질 평균**: 0.643 (3위)
- **적합한 용도**: 빠른 응답이 중요한 실시간 서비스 (단, 한국어 품질 트레이드오프 고려)

---

## 5. 권장 사항

### 5.1 모델 선정 권장

| 우선순위 | 모델 | 이유 |
|---------|------|------|
| 1순위 | **Qwen2.5-7B** | 품질 평가 최고점 (0.769), **KPI 목표 달성**, 속도-품질 균형 |
| 2순위 | Qwen2-7B | Correctness 최고 (0.666), 상세한 답변 (단, 속도 느림) |
| 3순위 | Llama3-8B | 응답 속도 최우선시 (품질은 미달) |

> **변경 사항 (130문항 전체 평가 반영)**:
> 파일럿 평가(15문항)에서는 Qwen2-7B가 1순위였으나, **130문항 전체 평가에서 Qwen2.5-7B가 역전**하였다.
> Qwen2.5-7B는 **Faithfulness(0.705)와 Relevancy(0.947) 모두에서 KPI 목표를 달성**한 유일한 모델이며,
> 응답 속도(2.94s)도 Qwen2-7B(3.82s) 대비 23% 빠르다.

### 5.2 상황별 권장 모델

| 시나리오 | 권장 모델 | 이유 |
|---------|----------|------|
| **기업 내부 RAG (메인)** | **Qwen2.5-7B** | KPI 달성, 속도-품질 균형 |
| 법률/규정 문의 | Qwen2-7B | 상세한 설명, Correctness 최고 |
| 일반 업무 지원 | Qwen2.5-7B | 균형 잡힌 성능 |
| 교육 콘텐츠 | Qwen2-7B | 충분한 설명 필요 |
| 실시간 챗봇 (속도 우선) | Llama3-8B | 빠른 응답 (품질 트레이드오프) |

---

## 6. 추가 고려사항

### 6.1 한국어 성능
- Qwen 계열이 한국어 처리에서 더 자연스러운 답변 생성
- Llama3는 영어 기반이므로 한국어 표현이 다소 어색할 수 있음

### 6.2 향후 개선 방향
1. ~~**RAGAS 정량 평가 추가**: Faithfulness, Answer Relevancy 등 품질 지표 측정~~ ✅ 완료
2. ~~**샘플 크기 확대**: 전체 130개로 확대하여 신뢰도 향상~~ ✅ 완료
3. **Human Evaluation**: 실제 사용자 만족도 조사
4. **Fine-tuning 검토**: 기업 도메인에 맞는 모델 미세조정
5. **제3 Judge LLM 교차검증**: GPT-4 또는 Claude 사용하여 편향 검증

### 6.3 이번 평가의 제한사항

- **Q세트 크기 및 편향**
  - 총 130문항으로, 사규/복무·교육 도메인에 집중되어 있어
    개발자용 기술 문서, 보안 사고 대응 시나리오 등은 별도 평가가 필요하다.

- **정확도/충실성 평가 (2단계에서 수행)** ✅
  - LLM-as-Judge 방식으로 Faithfulness, Relevancy, Correctness 평가 완료
  - **전체 130문항에 대한 완전 평가** 완료 (파일럿 15문항 → 130문항 확대)

- **Judge LLM 편향 가능성** ⚠️
  - 품질 평가는 **Qwen2.5-7B를 Judge LLM으로 사용**하여 수행했기 때문에,
    동일 계열(Qwen2, Qwen2.5) 모델에 **유리한 편향이 존재할 수 있다.**
  - 따라서 본 결과는 **"Qwen 계열 내 상대 비교"에는 유효**하지만,
    타 계열(Llama3 등)과의 **절대적인 우열을 단정하는 근거로 사용하기에는 한계**가 있다.
  - 향후 GPT-4 또는 Claude 등 제3의 Judge LLM을 사용한 교차 검증이 필요하다.

- **안전성 미평가**
  - PII 재노출, 내부 정책 위반 안내, 부적절한 표현 등은 별도 체크리스트를 만들어
    2차 평가에서 검증할 예정이며, 이번 결과만으로 LLM의 안전성을 단정할 수 없다.

- **단일 프롬프트 템플릿**
  - 하나의 공통 프롬프트 템플릿으로만 평가하여,
    모델별로 최적화된 프롬프트를 사용했을 때의 잠재 성능은 반영되지 않았다.

---

## 7. 보안·안전 관점 관찰 (1차)

- 이번 평가는 주로 응답 시간과 답변 길이에 초점을 맞추었으며,
  민감정보(PII) 재노출, 내부 정보 과다 노출, 정책 위반 안내 등의 **안전성 지표는 정량 평가하지 않았다.**

- 다만, 샘플 답변 검토 결과:
  - 세 모델 모두 개인정보·성희롱·직장 내 괴롭힘 등의 민감 주제에서
    기본적인 주의 문구(신고 절차, 회사 정책 준수 필요성 등)는 포함하는 경향을 보였다.
  - 반면, "어디까지 구체적으로 말해도 되는지" 수준은 모델·질문에 따라 편차가 있어,
    향후 **안전성 전용 Q세트 + 휴먼 평가**가 필요하다.

---

## 8. 결론

**최종 권장 모델: Qwen/Qwen2.5-7B-Instruct**

- **품질 평가 최고점** (평균 0.769) - Faithfulness 0.705 ✅, Relevancy 0.947 ✅
- **KPI 목표 달성**: 유일하게 Faithfulness ≥ 0.7, Relevancy ≥ 0.8 모두 충족
- 응답 속도 2.94s로 Qwen2-7B 대비 23% 빠름
- 답변 길이 469자로 적절한 상세도 유지
- 100% 성공률로 안정적인 서비스 가능
- 한국어 지원이 우수한 Qwen 계열

> **변경 근거 (130문항 전체 평가 반영)**:
> 파일럿 평가(15문항)에서는 Qwen2-7B가 1순위였으나, **130문항 전체 평가에서 Qwen2.5-7B가 역전**하였다.
> 전체 데이터셋에서 Qwen2.5-7B가 **Faithfulness(컨텍스트 충실도)와 Relevancy(답변 관련성)** 모두에서
> 가장 높은 점수를 기록하며, 설정한 **KPI 목표를 달성한 유일한 모델**로 확인되었다.

| 지표 | Qwen2.5-7B | Qwen2-7B | 차이 |
|------|------------|----------|------|
| Faithfulness | **0.705** ✅ | 0.653 | +0.052 |
| Relevancy | **0.947** ✅ | 0.938 | +0.009 |
| Correctness | 0.655 | **0.666** | -0.011 |
| 응답 시간 | **2.94s** | 3.82s | -0.88s |
| 품질 평균 | **0.769** | 0.752 | +0.017 |

**결론**: Qwen2.5-7B가 **품질(0.769)과 속도(2.94s) 모두에서 균형 잡힌 최적의 성능**을 보여주며,
기업 내부 RAG 시스템의 메인 모델로 **최종 권장**한다.

---

## 부록: 샘플 Q&A 비교 예시

아래는 동일한 질문에 대해 세 모델이 생성한 답변을 비교한 예시이다.
이를 통해 LLM-as-Judge 평가가 어떤 기준으로 점수를 부여하는지 직관적으로 이해할 수 있다.

### 예시 1: 성희롱예방교육 도메인 (SHP-001)

**질문**: "직장 내 성희롱은 무엇을 의미하며, 어떤 행동들이 포함되나요?"

**모범답안 (Ground Truth)**:
> 직장 내 성희롱은 업무 관계에서 지위를 이용하거나 성과 관련된 언행으로 상대방에게 성적 굴욕감·혐오감을 느끼게 하고, 근무 환경을 악화시키는 모든 행위를 말합니다. 불필요한 신체 접촉, 외모·사생활에 대한 성적 농담, 음란물 공유, 회식 자리에서의 과도한 술 강요 등이 대표적인 예입니다.

---

#### Qwen2-7B 답변 (권장 모델)

> 직장 내 성희롱은 사업주나 상급자가 자신의 지위를 이용하거나 업무와 관련하여 다른 근로자에게 성적 언동 등을 통해 성적 굴욕감이나 혐오감을 느끼게 하는 행동을 말합니다. 또한, 이러한 성희롱에 대한 반응을 무시하고 근로조건 및 고용에서 불이익을 주는 것도 포함됩니다.
>
> 직장 내 성희롱의 행동들은 다음과 같습니다:
> 1. 성적 언동
> 2. 성적 요구
> 3. 성적 비하 또는 성적 부당 대우
> 4. 성적 핵심화
> 5. 성적 장난

**특징**: 정의와 구체적인 유형을 체계적으로 나열하여 **가장 상세한 답변** 제공

---

#### Qwen2.5-7B 답변

> 직장 내 성희롱은 사업주, 상급자 또는 근로자가 직장 내 지위를 이용하거나 업무와 관련하여 다른 근로자에게 성적 언동 등으로 성적 굴욕감 또는 혐오감을 느끼게 하거나, 성적 언동 또는 그 밖의 요구 등에 따르지 아니하였다는 이유로 근로조건 및 고용에서 불이익을 주는 것을 의미합니다.
>
> 성희롱으로 포함되는 행동들:
> 1. 업무 관련성 있는 성적 언동
> 2. 직장 내 지위를 이용한 성적 요구나 압박
> 3. 상대방이 원하지 않는 성적 언동 또는 요구
> 4. 이익 공여의 의사 표시를 하는 행위

**특징**: 법적 정의에 충실하나 **구체적 예시가 부족**

---

#### Llama3-8B 답변

> 참고 문서에 따르면, "직장 내 성희롱"은 "사업주·상급자 또는 근로자가 직장 내의 지위를 이용하거나 업무와 관련하여 다른 근로자에게 성적 언동 등으로 성적 굴욕감 또는 혐오감을 느끼게 하거나 성적 언동 또는 그 밖의 요구 등에 따르지 아니하였다는 이유로 근로조건 및 고용에서 불이익을 주는 것을 말한다."는 정의를 가지고 있습니다.
>
> 따라서, 직장 내 성희롱은 업무관련성 있는 성적 언동 등으로 성적 굴욕감 또는 혐오감을 느끼게 하거나 성적 언동 또는 그 밖의 요구 등에 따르지 아니하였다는 행위를 포함합니다.

**특징**: 문서 원문 인용 위주로 **독자적 해석 없이 짧은 답변**

---

### 평가 점수 비교 (해당 도메인 유사 질문 기준 추정)

| 모델 | Faithfulness | Relevancy | Correctness | 총평 |
|------|--------------|-----------|-------------|------|
| **Qwen2-7B** | 높음 | 높음 | 높음 | 상세하고 체계적인 답변 |
| Qwen2.5-7B | 중간 | 중간 | 중간 | 정의 중심, 예시 부족 |
| Llama3-8B | 낮음 | 중간 | 낮음 | 원문 인용 위주, 설명 부족 |

> **해석**: Qwen2-7B는 컨텍스트 정보를 바탕으로 구조화된 답변을 생성하여 품질 점수가 높게 평가됨.
> Llama3-8B는 한국어 문서의 해석과 재구성 능력이 상대적으로 부족하여 낮은 점수를 받음.

---

**작성일**: 2025-12-09
**파일 위치**: `sample/eval/results/LLM_모델_비교_보고서.md`
**품질 평가 결과**: `sample/eval/results/quality_evaluation_20251209_172434.json`
