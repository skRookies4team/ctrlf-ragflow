"""
RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹(Chunking) + add_chunk
HWP / PDF / PPT / DOCX / TXT / CSV ìë™ ì²˜ë¦¬ + ë¬¸ì„œ íƒ€ì… íŒë³„ + ìë™ íŒ¨í„´ ê°ì§€ ì™„ì „íŒ
"""

import sys
import os
import time
import requests
import re
import json
import csv
import pdfplumber
from pathlib import Path
from typing import List, Sequence
from dotenv import load_dotenv
from difflib import SequenceMatcher
import google.generativeai as genai  # Gemini SDK

# =======================
# 0. ê²½ë¡œ/í™˜ê²½ ì„¤ì •
# =======================
BASE_DIR = Path(__file__).resolve().parent.parent

# ragflow ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ëª¨ë“ˆ ê²½ë¡œì— ì¶”ê°€ â†’ preprocessing íŒ¨í‚¤ì§€ import ê°€ëŠ¥
sys.path.insert(0, str(BASE_DIR))

load_dotenv(BASE_DIR / ".env")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# milvus í™˜ê²½ì„¤ì •
MILVUS_HOST = os.getenv("MILVUS_HOST")
MILVUS_PORT = os.getenv("MILVUS_PORT")
MILVUS_COLLECTION = os.getenv("MILVUS_COLLECTION")


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# RAGFLOW_EMBEDDING_MODEL ì´ "text-embedding-004@Gemini" ë¼ê³  ë˜ì–´ ìˆìœ¼ë‹ˆê¹Œ,
# ì‹¤ì œ Gemini ëª¨ë¸ ì´ë¦„ì€ ì•„ë˜ì²˜ëŸ¼ ì“¸ê²Œ.
GEMINI_EMBED_MODEL = os.getenv(
    "GEMINI_EMBED_MODEL",
    "models/text-embedding-004",
)
# ë²¡í„° ì°¨ì› (Milvus dimê³¼ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨)
GEMINI_EMBED_DIM = int(os.getenv("GEMINI_EMBED_DIM", "768"))

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("âš  GEMINI_API_KEY ê°€ .envì— ì—†ìŠµë‹ˆë‹¤. embed_text() í˜¸ì¶œ ì‹œ ì—ëŸ¬ê°€ ë‚©ë‹ˆë‹¤.")


# =======================
# 1. RAGFlow SDK import
# =======================
try:
    from ragflow_sdk import RAGFlow
except ImportError:
    # sdk/python í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ í›„ ì¬ì‹œë„
    sys.path.insert(0, str(BASE_DIR / "sdk" / "python"))
    from ragflow_sdk import RAGFlow

from ragflow_sdk.modules.dataset import DataSet
from milvus_proxy import MilvusProxy

# =======================
# 2. ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ëª¨ë“ˆ import
# =======================
from preprocessing.coverters.hwp_to_docx import HwpAdapter
from preprocessing.classifier.document_classifier import DocumentClassifier
from preprocessing.pipeline import PreprocessPipeline

# =======================
# 3. í™˜ê²½ ë³€ìˆ˜
# =======================
HOST_ADDRESS = os.getenv("RAGFLOW_HOST", "http://localhost")
API_KEY = os.getenv("RAGFLOW_API_KEY")
EMBEDDING_MODEL = os.getenv(
    "RAGFLOW_EMBEDDING_MODEL",
    "text-embedding-004@Gemini"
)

if not API_KEY:
    print("âŒ RAGFLOW_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    sys.exit(1)

# =======================
# 4. ê³µìš© ê°ì²´
# =======================
hwp_adapter = HwpAdapter()
classifier = DocumentClassifier()
preprocess_pipeline = PreprocessPipeline()


# ===========================================================
# ì¶œë ¥ ìœ í‹¸
# ===========================================================
def print_section(title: str):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_step(n: int, text: str):
    print(f"\n[ë‹¨ê³„ {n}] {text}")
    print("-" * 60)


# ===========================================================
# 1. ë¬¸ì„œ íƒ€ì… ìë™ íŒë‹¨ (í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·œì •/ë°©ì¹¨ íŒë³„ìš©)
# ===========================================================
def detect_document_type(raw_text: str) -> str:
    """
    ë¬¸ì„œ í˜•íƒœ ìë™ íŒë‹¨
    - regulation: 'ì œ nì¡°'ê°€ ë§ì´ ë“±ì¥í•˜ëŠ” ê·œì •ë¥˜
    - structured: â—‡/â—Š/â—‹/ìˆ«ì í—¤ë”ê°€ ë§ì€ ë³´ì•ˆ/ë°©ì¹¨ ë¬¸ì„œ
    - general: ì¼ë°˜ ë¬¸ì„œ (ë³´ê³ ì„œ, íšŒì˜ë¡ ë“±)
    """
    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    article = sum(1 for l in lines if re.match(r"^\s*ì œ\s*\d+\s*ì¡°", l))
    diamond = sum(1 for l in lines if re.match(r"^\s*[â—Šâ—†â—‡]", l))
    circle = sum(1 for l in lines if re.match(r"^\s*[â—‹â—â—¯O]", l))
    number = sum(1 for l in lines if re.match(r"^\s*\d+\.", l))

    paragraph_count = len([p for p in text.split("\n\n") if p.strip()])
    length = len(text)

    if article >= 3:
        return "regulation"

    if article == 0 and (diamond >= 2 or circle >= 2):
        return "structured"

    if number >= 5:
        return "structured"

    if length > 2000 and paragraph_count >= 3:
        return "general"

    return "general"


# ===========================================================
# 2. heading íŒ¨í„´ ìë™ ê°ì§€
# ===========================================================
def detect_heading_patterns_from_text(raw_text: str) -> List[str]:
    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    candidate = {
        "article": r"^\s*ì œ\s*\d+\s*ì¡°",
        "diamond": r"^\s*[â—Šâ—†â—‡]\s*",
        "circle": r"^\s*[â—‹â—â—¯O]\s*",
        "special": r"^\s*[ä»¤â€»]\s*",
        "number_top": r"^\s*\d+\.\s+",
        "number_sub": r"^\s*\d+\.\d+\s+",
    }

    counts = {k: 0 for k in candidate.keys()}
    compiled = {k: re.compile(v) for k, v in candidate.items()}

    for l in lines:
        s = l.rstrip()
        for k, pat in compiled.items():
            if pat.match(s):
                counts[k] += 1

    article = counts["article"]
    diamond = counts["diamond"]
    circle = counts["circle"]
    special = counts["special"]
    num = counts["number_top"] + counts["number_sub"]

    # ê·œì •ë¥˜
    if article >= 3:
        active = ["article"]

    # ë³´ì•ˆê´€ë¦¬ê·œì •
    elif article == 0 and diamond >= 1:
        active = ["diamond"]
        if circle + special >= 1:
            active += ["circle", "special"]

    # ê²½ì˜ë°©ì¹¨, ìˆ«ì êµ¬ì¡° ë¬¸ì„œ
    elif num >= 3:
        active = ["number_top", "number_sub"]

    else:
        nonzero = [k for k, v in counts.items() if v > 0]
        if nonzero:
            max_cnt = max(counts[k] for k in nonzero)
            active = [k for k in nonzero if counts[k] == max_cnt]
        else:
            active = ["article"]

    return [candidate[k] for k in active]


# ===========================================================
# 3. ê¸¸ì´ ê¸°ë°˜ ìŠ¤í”Œë¦¿ í•¨ìˆ˜
# ===========================================================
def split_long_chunk_with_heading(chunk_text: str, max_chars: int) -> List[str]:
    if len(chunk_text) <= max_chars:
        return [chunk_text]

    lines = chunk_text.splitlines()
    heading = lines[0]
    body = "\n".join(lines[1:]).strip()
    paras = [p.strip() for p in body.split("\n\n") if p.strip()]

    max_body = max_chars - len(heading) - 10
    chunks: List[str] = []
    buf: List[str] = []

    def flush():
        nonlocal buf
        if not buf:
            return
        text = heading + "\n" + "\n\n".join(buf)
        chunks.append(text)
        buf = []

    for p in paras:
        if not buf:
            cand = p
        else:
            cand = "\n\n".join(buf + [p])

        if len(cand) <= max_body:
            buf.append(p)
        else:
            flush()
            if len(p) > max_body:
                # ê°•ì œ ìë¥´ê¸°
                s = 0
                while s < len(p):
                    part = p[s:s + max_body]
                    chunks.append(heading + "\n" + part)
                    s += max_body
            else:
                buf = [p]

    flush()
    return chunks


# ===========================================================
# 4. ê·œì •í˜• ì²­í‚¹
# ===========================================================
def split_text_by_rules(
    raw_text: str,
    heading_patterns: Sequence[str],
    max_chars: int,
    strict_heading_only: bool = False
) -> List[str]:
    """
    strict_heading_only=True â†’ ê¸¸ì´ ê¸°ì¤€ ë¶„í•  OFF (ì¡° ë‹¨ìœ„ ìœ ì§€)
    """
    lines = raw_text.splitlines()
    compiled = [re.compile(p) for p in heading_patterns]
    coarse: List[str] = []
    buf: List[str] = []

    def is_heading(line: str) -> bool:
        s = line.strip()
        for pat in compiled:
            if pat.match(s):
                return True
        return False

    def flush():
        nonlocal buf
        if not buf:
            return None
        t = "\n".join(buf).strip()
        buf = []
        return t

    for line in lines:
        if is_heading(line):
            f = flush()
            if f:
                coarse.append(f)
            buf = [line]
        else:
            buf.append(line)

    last = flush()
    if last:
        coarse.append(last)

    # ê¸¸ì´ ê¸°ë°˜ ë¶„í•  í•´ì œ (ê·œì •ë¥˜)
    if strict_heading_only:
        return [c for c in coarse if len(c.strip()) > 20]

    # structured ë¬¸ì„œ â†’ í•„ìš”ì‹œ ê¸¸ì´ ë¶„í• 
    final: List[str] = []
    for ch in coarse:
        if len(ch) <= max_chars:
            final.append(ch)
        else:
            final.extend(split_long_chunk_with_heading(ch, max_chars))

    return [c for c in final if len(c.strip()) > 20]
# ==========================================
# HWP / ìŠ¬ë¼ì´ë“œí˜• PDF ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë˜í¼
# ==========================================
def preprocess_to_chunks(path: Path, chunk_size: int = 1200) -> list[str]:
    """
    PreprocessPipelineì„ ì‹¤í–‰í•´ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸(list[str])ë¡œ ë³€í™˜.
    - result êµ¬ì¡°:
        {
          "run_id": "...",
          "page_count": ...,
          "avg_quality": ...,
          "pages": [...],
          "chunks": [
             {"text": "...", "page_index": ..., ...},
             ...
          ]
        }
    ë¥¼ ê°€ì •í•˜ê³  ì•ˆì „í•˜ê²Œ íŒŒì‹±.
    """
    result = preprocess_pipeline.run(str(path), chunk_size=chunk_size)

    # 1) resultê°€ ë¬¸ìì—´ì´ë©´ â†’ JSON íŒŒì¼ ê²½ë¡œë¼ê³  ê°€ì •í•˜ê³  ë¡œë“œ
    if isinstance(result, str):
        try:
            with open(result, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            return [result.strip()] if result.strip() else []
    else:
        data = result

    # 2) dataì—ì„œ ì‹¤ì œ chunk ëª©ë¡ êº¼ë‚´ê¸°
    items = []

    if isinstance(data, dict):
        # case 1: {"result_json": {...}} (êµ¬ë²„ì „ í˜¸í™˜)
        if "result_json" in data:
            rj = data["result_json"]
            if isinstance(rj, dict):
                if "chunks" in rj and isinstance(rj["chunks"], list):
                    items = rj["chunks"]
            elif isinstance(rj, list):
                items = rj
        # case 2: {"chunks": [...]} (í˜„ì¬ ë²„ì „)
        elif "chunks" in data and isinstance(data["chunks"], list):
            items = data["chunks"]

    elif isinstance(data, list):
        items = data

    if not items:
        return []

    # 3) ê° itemì—ì„œ í…ìŠ¤íŠ¸ë§Œ ë½‘ê¸°
    chunks: list[str] = []
    for item in items:
        if isinstance(item, dict):
            text = item.get("text") or item.get("content") or ""
        else:
            text = str(item)

        text = text.strip()
        if text:
            chunks.append(text)

    return chunks


# =========================
# CER(ë¬¸ì ì˜¤ë¥˜ìœ¨) ê³„ì‚° ìœ í‹¸ (ì„ íƒì )
# =========================
def cer(pred: str, truth: str) -> float:
    """
    CER = (ì‚½ì… + ì‚­ì œ + êµì²´) / ì •ë‹µ ê¸€ì ìˆ˜
    edit distance ê¸°ë°˜
    """
    import numpy as np

    p = list(pred)
    t = list(truth)

    dp = np.zeros((len(t) + 1, len(p) + 1), dtype=int)

    for i in range(len(t) + 1):
        dp[i][0] = i
    for j in range(len(p) + 1):
        dp[0][j] = j

    for i in range(1, len(t) + 1):
        for j in range(1, len(p) + 1):
            cost = 0 if t[i - 1] == p[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,         # ì‚­ì œ
                dp[i][j - 1] + 1,         # ì‚½ì…
                dp[i - 1][j - 1] + cost   # êµì²´
            )

    return dp[len(t)][len(p)] / max(1, len(t))


def eval_cer_for_pdf_text(pdf_path: Path, extracted_text: str) -> None:
    """
    ì›ë³¸ PDFì— ëŒ€ì‘í•˜ëŠ” ì •ë‹µ í…ìŠ¤íŠ¸(.txt)ë¥¼ ì½ì–´ì„œ CER ì¶œë ¥.

    ì •ë‹µ íŒŒì¼ ìœ„ì¹˜:
        sample/dataset/solution/<pdfíŒŒì¼ëª…>.txt
        ì˜ˆ) ì´ì‚¬íšŒê·œì •.pdf â†’ solution/ì´ì‚¬íšŒê·œì •.txt
    """
    gt_root = pdf_path.parent / "solution"
    gt_path = gt_root / f"{pdf_path.stem}.txt"

    if not gt_path.exists():
        print(f"   âš  CER ìŠ¤í‚µ: ì •ë‹µ íŒŒì¼ ì—†ìŒ â†’ {gt_path}")
        return

    truth = gt_path.read_text(encoding="utf-8", errors="ignore")
    pred = extracted_text

    truth_norm = truth.replace("\r\n", "\n").strip()
    pred_norm = pred.replace("\r\n", "\n").strip()

    score = cer(pred_norm, truth_norm)
    print(f"   âœ… CER í‰ê°€ ê²°ê³¼: {score * 100:.2f}% (ë¬¸ì ì˜¤ë¥˜ìœ¨)")
    print(f"      â†’ ë¬¸ì ì •í™•ë„(ëŒ€ëµ): {(1 - score) * 100:.2f}%")


# ===========================================================
# 5. DOCX / TXT / CSV ì „ìš© chunk í•¨ìˆ˜
#    (PDF/HWP/PPTëŠ” ìƒìœ„ ë£¨í”„ì—ì„œ ì²˜ë¦¬)
# ===========================================================
def extract_text_docx(path: Path) -> str:
    from docx import Document
    doc = Document(str(path))
    paras = [p.text for p in doc.paragraphs if p.text.strip()]
    return "\n".join(paras)


def extract_text_txt(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")


def extract_text_csv(path: Path) -> str:
    """
    CSV íŒŒì¼ì„ TEXT ë¬¸ì„œì²˜ëŸ¼ ë³€í™˜í•˜ì—¬ ë°˜í™˜.
    ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ "col: value" í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
    """
    lines: list[str] = []

    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        reader = csv.reader(f)
        rows = list(reader)

    if not rows:
        return ""

    header = rows[0]
    for row in rows[1:]:
        for col, val in zip(header, row):
            col_s = str(col).strip()
            val_s = str(val).strip()
            if col_s or val_s:
                lines.append(f"{col_s}: {val_s}")
        lines.append("")  # í–‰ê³¼ í–‰ ì‚¬ì´ ê³µë°± ë¼ì¸

    return "\n".join(lines).strip()


def chunk_document(path: Path) -> List[str]:
    """
    DOCX / TXT / CSV ì „ìš© ì²­í‚¹
    (PDF/HWP/HWPX/PPT ëŠ” ìƒìœ„ for ë£¨í”„ì—ì„œ ë³„ë„ ì²˜ë¦¬)
    """
    ext = path.suffix.lower()

    if ext == ".docx":
        raw = extract_text_docx(path)
    elif ext == ".csv":
        raw = extract_text_csv(path)
    else:
        raw = extract_text_txt(path)

    if not raw.strip():
        return []

    # ë¬¸ì„œ íƒ€ì… ë¶„ì„ í›„ ê¸°ì¡´ ê·œì • ì²­í‚¹
    doc_type = detect_document_type(raw)
    patterns = detect_heading_patterns_from_text(raw)

    if doc_type == "regulation":
        return split_text_by_rules(raw, patterns, max_chars=999999, strict_heading_only=True)
    elif doc_type == "structured":
        return split_text_by_rules(raw, patterns, max_chars=2000)
    else:
        # ì¼ë°˜ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼
        paras = [p.strip() for p in raw.split("\n\n") if p.strip()]
        chunks: List[str] = []
        buf: List[str] = []

        for p in paras:
            candidate = "\n\n".join(buf + [p]) if buf else p
            if len(candidate) <= 800:
                buf.append(p)
            else:
                if buf:
                    chunks.append("\n\n".join(buf))
                buf = [p]
        if buf:
            chunks.append("\n\n".join(buf))
        return chunks


def chunk_text_pdf(path: Path) -> list[str]:
    """
    í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF â†’ pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„
    DOCXë‘ ë˜‘ê°™ì€ ê·œì •/êµ¬ì¡° ë¬¸ì„œ ì²­í‚¹ ë¡œì§ ì ìš©
    """
    pages: list[str] = []
    with pdfplumber.open(str(path)) as pdf:
        for p in pdf.pages:
            pages.append(p.extract_text() or "")

    raw = "\n".join(pages)

    # ë¬¸ì„œ íƒ€ì… ë¶„ì„
    doc_type = detect_document_type(raw)
    patterns = detect_heading_patterns_from_text(raw)

    if doc_type == "regulation":
        # ì œ 1 ì¡°, ì œ 2 ì¡° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê¸¸ì´ëŠ” ì›¬ë§Œí•˜ë©´ ì•ˆ ìë¦„
        return split_text_by_rules(raw, patterns, max_chars=999999, strict_heading_only=True)
    elif doc_type == "structured":
        # ìˆ«ì í—¤ë”/ëª©ì°¨ê°€ ë§ì€ ê²½ìš°: ì¡°ê¸ˆ ë” ì˜ê²Œ
        return split_text_by_rules(raw, patterns, max_chars=2000)
    else:
        # ì¼ë°˜ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ â†’ DOCXë‘ ë™ì¼í•œ ë‹¨ë½ ê¸°ë°˜ ì²­í‚¹
        paras = [p.strip() for p in raw.split("\n\n") if p.strip()]
        chunks: list[str] = []
        buf: list[str] = []

        for p in paras:
            candidate = "\n\n".join(buf + [p]) if buf else p
            if len(candidate) <= 800:
                buf.append(p)
            else:
                if buf:
                    chunks.append("\n\n".join(buf))
                buf = [p]
        if buf:
            chunks.append("\n\n".join(buf))

        return chunks
# ===========================================================
# ë©”ì¸ ì„¤ì •ê°’ & ë„ë©”ì¸ ë””ë ‰í„°ë¦¬ ì •ì˜
# ===========================================================
MAX_CHUNK_LEN = 8000  # ë„ˆë¬´ ê¸´ ì²­í¬ ë°©ì§€ìš© (í•„ìš”í•˜ë©´ 4000~6000 ì •ë„ë¡œ ì¤„ì—¬ë„ ë¨)

# =========================
# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜ (Milvus ìš©, Gemini text-embedding-004)
# =========================
def embed_text(text: str) -> list[float]:
    """
    Gemini text-embedding-004ì„ í˜¸ì¶œí•´ì„œ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.
    - dim: ê¸°ë³¸ 768 (GEMINI_EMBED_DIMê³¼ MilvusProxy dimì´ ë°˜ë“œì‹œ ê°™ì•„ì•¼ í•¨)
    """
    if not GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ê¸´ ë¬¸ì„œëŠ” ì˜ë¼ì„œ í‰ê· ë‚´ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ ì „ì²˜ë¦¬í•˜ë©´ ë¨.
    # ì¼ë‹¨ì€ ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—Embedding.
    response = genai.embed_content(
        model=GEMINI_EMBED_MODEL,
        content=text,
        task_type="RETRIEVAL_DOCUMENT",
    )

    # text-embedding-004 ì‘ë‹µì€ {"embedding": [...]} í˜•íƒœ
    embedding = response.get("embedding") or response["embeddings"][0]

    if len(embedding) != GEMINI_EMBED_DIM:
        # ì˜ˆìƒ ì°¨ì›ê³¼ ë‹¤ë¥´ë©´ ê²½ê³ ë§Œ ì°ê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
        print(
            f"âš  Gemini ì„ë² ë”© ì°¨ì›({len(embedding)})ì´ "
            f"GEMINI_EMBED_DIM({GEMINI_EMBED_DIM})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤."
        )

    return embedding

SCRIPT_DIR = Path(__file__).parent  # sample í´ë” ê¸°ì¤€

DOMAIN_DIRS = {
    "ì§ë¬´êµìœ¡":        SCRIPT_DIR / "dataset_ì§ë¬´êµìœ¡",
    "ì¥ì• ì¸ì¸ì‹ê°œì„ êµìœ¡": SCRIPT_DIR / "dataset_ì¥ì• ì¸ì¸ì‹ê°œì„ ",
    "ì§ì¥ë‚´ê´´ë¡­í˜êµìœ¡": SCRIPT_DIR / "dataset_ê´´ë¡­í˜êµìœ¡",
    "ì§ì¥ë‚´ì„±í¬ë¡±êµìœ¡": SCRIPT_DIR / "dataset_ì„±í¬ë¡±êµìœ¡",
    "ì •ë³´ë³´ì•ˆêµìœ¡":    SCRIPT_DIR / "dataset_ì •ë³´ë³´ì•ˆêµìœ¡",
    "ì‚¬ë‚´ê·œì •":        SCRIPT_DIR / "dataset_ì‚¬ë‚´ê·œì •",
}


def compare_with_solution(dataset_dir: Path, fpath: Path, chunks: list[str]):
    solution_dir = dataset_dir / "solution"
    solution_txt_path = solution_dir / f"{fpath.stem}.txt"

    if not solution_txt_path.exists():
        print(f"  [ìœ ì‚¬ë„] solution txt ì—†ìŒ â†’ {solution_txt_path.name} (ìŠ¤í‚µ)")
        return

    try:
        solution_text = solution_txt_path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        solution_text = solution_txt_path.read_text(encoding="cp949")

    chunk_text = "\n".join(c for c in chunks if c and c.strip())

    sim = SequenceMatcher(None, chunk_text, solution_text).ratio()

    print(f"  [ìœ ì‚¬ë„] ì „ì²´ ì²­í‚¹ ê²°ê³¼ vs solution/{solution_txt_path.name}: {sim*100:.2f}%")


# ===========================================================
# ì²­í¬ ì¶”ê°€ ìœ í‹¸ (RAGFlow + (ì˜µì…˜) Milvus ë™ì‹œ ì €ì¥ ë²„ì „)
# ===========================================================
def add_chunks_safe(
    doc,
    chunks,
    milvus: "MilvusProxy | None" = None,
    dataset_id: str | None = None,   # â† Milvusì— ë“¤ì–´ê°ˆ dataset ì‹ë³„ì (ìš°ë¦¬ëŠ” domain ì´ë¦„ ë„£ì„ ê±°)
    doc_id: str | None = None,       # â† íŒŒì¼ ì´ë¦„
):
    """
    ì²­í¬ë“¤ì„ RAGFlow docì— ì¶”ê°€ + (ì„ íƒ) Milvusì—ë„ í•¨ê»˜ ì €ì¥.
    - RAGFlow: doc.add_chunk(content=...)
    - Milvus:  dataset_id / doc_id / chunk_id / text / embedding
    """
    print(f"â†’ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

    added = 0
    milvus_payload = []

    for idx, c in enumerate(chunks, start=1):
        if not c or not c.strip():
            continue

        # 1) RAGFlowì— ì²­í¬ ì¶”ê°€
        doc.add_chunk(content=c)
        added += 1

        # 2) Milvusìš© payload ì¤€ë¹„ (ì„ë² ë”©ê¹Œì§€)
        if milvus is not None and dataset_id is not None and doc_id is not None:
            try:
                emb = embed_text(c)   # â† Gemini ì„ë² ë”© í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì´ë¯¸ ìœ„ì— stub ë§Œë“¤ì–´ë‘” ê·¸ê±°)
                milvus_payload.append(
                    {
                        "doc_id": doc_id,
                        "chunk_id": idx,
                        "text": c,
                        "embedding": emb,
                    }
                )
            except NotImplementedError:
                # embed_text ì•„ì§ êµ¬í˜„ ì•ˆ í–ˆìœ¼ë©´ ê²½ê³ ë§Œ ì°ê³  ì´í›„ë¶€í„´ ì‹œë„ ì•ˆ í•¨
                if idx == 1:
                    print("âš  embed_text()ê°€ êµ¬í˜„ë˜ì§€ ì•Šì•„ Milvus ì €ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                milvus = None  # í•œ ë²ˆë§Œ ê²½ê³  ì°ê³  ì´í›„ì—” ì‹œë„ ì•ˆ í•¨

        # ë¯¸ë¦¬ë³´ê¸°ëŠ” ì•ì˜ ë‘ ê°œë§Œ
        if idx <= 2:
            print(f"\n  [ë¯¸ë¦¬ë³´ê¸° ì²­í¬ {idx}]")
            preview = c[:200]
            if len(c) > 200:
                preview += "..."
            print(preview)

    print(f"â†’ ì´ {added}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")

    # 3) Milvusì— í•œ ë²ˆì— insert
    if milvus is not None and milvus_payload:
        milvus.insert_chunks(dataset_id=dataset_id, chunks=milvus_payload)
        
# ===========================================================
# ë©”ì¸
# ===========================================================
def main():
    print_section("RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹ + add_chunk (HWP/PDF/PPT/DOCX/TXT/CSV í¬í•¨)")

    # ------------------------------------
    # 1) ì„œë²„ ì—°ê²°
    # ------------------------------------
    print_step(1, "ì„œë²„ ì—°ê²°")
    try:
        _ = requests.get(
            f"{HOST_ADDRESS}/api/v1/datasets",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=5,
        )
        rag = RAGFlow(API_KEY, HOST_ADDRESS)
        print("âœ… RAGFlow ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    # â˜… ì—¬ê¸° ì¶”ê°€: Milvus ì—°ê²°
        print_step(1, "Milvus ì—°ê²°")
    try:
        milvus = MilvusProxy(
            host=MILVUS_HOST,
            port=MILVUS_PORT,
            collection_name=MILVUS_COLLECTION,
            dim=GEMINI_EMBED_DIM,   # Gemini ì„ë² ë”© ì°¨ì›ê³¼ ë°˜ë“œì‹œ ë™ì¼
        )
        print("âœ… Milvus ì—°ê²°/ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨ (ì¼ë‹¨ RAGFlowë§Œ ì§„í–‰): {e}")
        milvus = None

    # ==============================================
    # â˜… ë„ë©”ì¸ë³„ë¡œ Datasetì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬ì„± â˜…
    # ==============================================
    for domain, dataset_dir in DOMAIN_DIRS.items():
        print("\n" + "#" * 60)
        print(f"### ë„ë©”ì¸: {domain}")
        print(f"### ë¡œì»¬ í´ë”: {dataset_dir}")
        print("#" * 60)

        if not dataset_dir.exists():
            print(f"âš ï¸  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µ: {dataset_dir}")
            continue

        # ------------------------------------
        # 2) ë„ë©”ì¸ í´ë” ì•ˆ íŒŒì¼ ìŠ¤ìº”
        # ------------------------------------
        print_step(2, f"[{domain}] dataset í´ë” ìŠ¤ìº”")

        pdfs = list(dataset_dir.glob("*.pdf"))
        ppts = list(dataset_dir.glob("*.ppt")) + list(dataset_dir.glob("*.pptx"))
        hwps = list(dataset_dir.glob("*.hwp")) + list(dataset_dir.glob("*.hwpx"))
        docxs = list(dataset_dir.glob("*.docx"))
        txts = list(dataset_dir.glob("*.txt"))
        csvs = list(dataset_dir.glob("*.csv"))

        files = sorted(pdfs + ppts + hwps + docxs + txts + csvs)

        if not files:
            print(f"âŒ [{domain}] ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        print("ğŸ“‚ ì²˜ë¦¬ íŒŒì¼:")
        for f in files:
            print("   -", f.name)

        # ------------------------------------
        # 3) ë„ë©”ì¸ë³„ Dataset ìƒì„±
        # ------------------------------------
        print_step(3, f"[{domain}] ë°ì´í„°ì…‹ ìƒì„±")
        dataset_name = f"auto_{domain}_{int(time.time())}"

        parser_config = DataSet.ParserConfig(rag, {"raptor": {"use_raptor": False}})

        dataset = rag.create_dataset(
            name=dataset_name,
            description=f"{domain} ì „ìš© ìë™ ì²­í‚¹ ë°ì´í„°ì…‹",
            chunk_method="manual",
            embedding_model=EMBEDDING_MODEL,
            parser_config=parser_config,
        )

        print(f"âœ… [{domain}] Dataset ìƒì„± ì™„ë£Œ: {dataset.id} (name={dataset_name})")

        # ------------------------------------
        # 4) íŒŒì¼ë³„ ì—…ë¡œë“œ + ì²­í‚¹
        # ------------------------------------
        print_step(4, f"[{domain}] íŒŒì¼ ì—…ë¡œë“œ + ì²­í‚¹")

        for fpath in files:
            fpath = fpath.resolve()
            ext = fpath.suffix.lower().lstrip(".")
            print(f"\n======= [{domain}] {fpath.name} ì²˜ë¦¬ =======")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 4-1. HWP/HWPX â†’ DOCXë¡œ ë³€í™˜
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if ext in ("hwp", "hwpx"):
                print(f"[HWP] {fpath.name} â†’ DOCXë¡œ ë³€í™˜")
                docx_path = hwp_adapter.to_docx(str(fpath))
                fpath = Path(docx_path)
                ext = "docx"

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 4-2. PDF / PPT / PPTX ì²˜ë¦¬
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if ext in ("pdf", "ppt", "pptx"):
                if ext == "pdf":
                    doc_type = classifier.classify(str(fpath))
                else:
                    doc_type = "ppt"

                print(f"â†’ ë¬¸ì„œ íƒ€ì…: {doc_type}")

                # 1-1) í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF â†’ ìš°ë¦¬ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©
                if doc_type == "text_pdf":
                    print("â†’ [í…ìŠ¤íŠ¸ PDF] ë¡œì»¬ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©")

                    with open(fpath, "rb") as fb:
                        blob = fb.read()

                    doc = dataset.upload_documents(
                        [{"display_name": fpath.name, "blob": blob}]
                    )[0]
                    print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

                    chunks = chunk_text_pdf(fpath)

                    # â˜… solution/ ì •ë‹µ txtê°€ ìˆìœ¼ë©´ ìœ ì‚¬ë„ ì²´í¬
                    compare_with_solution(dataset_dir, fpath, chunks)

                    # RAGFlow + (ì˜µì…˜) Milvus ë™ì‹œ ì €ì¥
                    add_chunks_safe(
                        doc,
                        chunks,
                        milvus=milvus,
                        dataset_id=domain,   # ë„ë©”ì¸ë³„ dataset ì´ë¦„ì„ dataset_idë¡œ ì‚¬ìš©
                        doc_id=fpath.name,
                    )
                    continue

                # 1-2) ì´ë¯¸ì§€ ê¸°ë°˜ PDF / PPT â†’ PreprocessPipeline + add_chunk
                print("â†’ [ì´ë¯¸ì§€/ìŠ¬ë¼ì´ë“œ] PreprocessPipeline + add_chunk ì‚¬ìš©")

                with open(fpath, "rb") as fb:
                    blob = fb.read()

                doc = dataset.upload_documents(
                    [{"display_name": fpath.name, "blob": blob}]
                )[0]
                print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

                pipeline_result = preprocess_pipeline.run(str(fpath))

                print("â†’ PreprocessPipeline ì™„ë£Œ")

                chunks = [c["text"] for c in pipeline_result.get("chunks", [])]
                print(f"â†’ íŒŒì´í”„ë¼ì¸ ì²­í¬ {len(chunks)}ê°œ ë°˜í™˜")

                add_chunks_safe(
                    doc,
                    chunks,
                    milvus=milvus,
                    dataset_id=domain,
                    doc_id=fpath.name,
                )
                continue

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 4-3. CSV / DOCX / TXT â†’ ê¸°ì¡´ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if ext in ("csv", "docx", "txt"):
                print("â†’ [CSV/DOCX/TXT] ê¸°ì¡´ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©")

                with open(fpath, "rb") as fb:
                    blob = fb.read()

                doc = dataset.upload_documents(
                    [{"display_name": fpath.name, "blob": blob}]
                )[0]
                print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

                chunks = chunk_document(fpath)
                compare_with_solution(dataset_dir, fpath, chunks)

                add_chunks_safe(
                    doc,
                    chunks,
                    milvus=milvus,
                    dataset_id=domain,
                    doc_id=fpath.name,
                )
                continue

            # ê¸°íƒ€ í™•ì¥ìëŠ” ìŠ¤í‚µ
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ìì…ë‹ˆë‹¤: .{ext} (ìŠ¤í‚µ)")

        # ------------------------------------
        # 5) ë„ë©”ì¸ë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨íˆ 1ë²ˆë§Œ)
        # ------------------------------------
        print_step(5, f"[{domain}] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")

        query = f"{domain} ê´€ë ¨ ë¬¸ì„œì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?"
        results = rag.retrieve(
            dataset_ids=[dataset.id],
            question=query,
            top_k=3,
        )

        for i, r in enumerate(results, 1):
            print(f"\n[ê²€ìƒ‰ {i}]")
            print(r.content[:200] + "...")


if __name__ == "__main__":
    main()
