#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹(Chunking) + add_chunk
PDF / DOCX / TXT ìë™ ì²˜ë¦¬ + ë¬¸ì„œ íƒ€ì… íŒë³„ + ìë™ íŒ¨í„´ ê°ì§€ ì™„ì „íŒ
"""

import os
import sys
import time
import requests
import re
from pathlib import Path
from typing import List, Sequence
from dotenv import load_dotenv
from ragflow_sdk.modules.dataset import DataSet

# =======================
# 0. RAGFlow SDK import
# =======================
try:
    from ragflow_sdk import RAGFlow
except ImportError:
    sys.path.insert(0, str(Path(__file__).parent.parent / "sdk" / "python"))
    from ragflow_sdk import RAGFlow

BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

HOST_ADDRESS = os.getenv("RAGFLOW_HOST", "http://localhost")
API_KEY = os.getenv("RAGFLOW_API_KEY")
EMBEDDING_MODEL = os.getenv(
    "RAGFLOW_EMBEDDING_MODEL",
    "text-embedding-004@Gemini"
)

if not API_KEY:
    print("âŒ RAGFLOW_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    sys.exit(1)


# ===========================================================
# ì¶œë ¥ ìœ í‹¸
# ===========================================================
def print_section(title):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_step(n, text):
    print(f"\n[ë‹¨ê³„ {n}] {text}")
    print("-" * 60)


# ===========================================================
# 1. ë¬¸ì„œ íƒ€ì… ìë™ íŒë‹¨
# ===========================================================
def detect_document_type(raw_text: str) -> str:
    """
    ë¬¸ì„œ í˜•íƒœ ìë™ íŒë‹¨
    - regulation: 'ì œ nì¡°'ê°€ ë§ì´ ë“±ì¥í•˜ëŠ” ê·œì •ë¥˜
    - structured: â—‡/â—Š/â—‹/ìˆ«ì í—¤ë”ê°€ ë§ì€ ë³´ì•ˆ/ë°©ì¹¨ ë¬¸ì„œ
    - general: ì¼ë°˜ ë¬¸ì„œ (ë³´ê³ ì„œ, íšŒì˜ë¡ ë“±)
    """

    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    article = sum(1 for l in lines if re.match(r"^\s*ì œ\s*\d+\s*ì¡°", l))
    diamond = sum(1 for l in lines if re.match(r"^\s*[â—Šâ—†â—‡]", l))
    circle = sum(1 for l in lines if re.match(r"^\s*[â—‹â—â—¯O]", l))
    number = sum(1 for l in lines if re.match(r"^\s*\d+\.", l))

    paragraph_count = len([p for p in text.split("\n\n") if p.strip()])
    length = len(text)

    if article >= 3:
        return "regulation"

    if article == 0 and (diamond >= 2 or circle >= 2):
        return "structured"

    if number >= 5:
        return "structured"

    if length > 2000 and paragraph_count >= 3:
        return "general"

    return "general"


# ===========================================================
# 2. heading íŒ¨í„´ ìë™ ê°ì§€
# ===========================================================
def detect_heading_patterns_from_text(raw_text: str) -> list[str]:
    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    candidate = {
        "article": r"^\s*ì œ\s*\d+\s*ì¡°",
        "diamond": r"^\s*[â—Šâ—†â—‡]\s*",
        "circle": r"^\s*[â—‹â—â—¯O]\s*",
        "special": r"^\s*[ä»¤â€»]\s*",
        "number_top": r"^\s*\d+\.\s+",
        "number_sub": r"^\s*\d+\.\d+\s+",
    }

    counts = {k: 0 for k in candidate.keys()}
    compiled = {k: re.compile(v) for k, v in candidate.items()}

    for l in lines:
        s = l.rstrip()
        for k, pat in compiled.items():
            if pat.match(s):
                counts[k] += 1

    article = counts["article"]
    diamond = counts["diamond"]
    circle = counts["circle"]
    special = counts["special"]
    num = counts["number_top"] + counts["number_sub"]

    # ê·œì •ë¥˜
    if article >= 3:
        active = ["article"]

    # ë³´ì•ˆê´€ë¦¬ê·œì •
    elif article == 0 and diamond >= 1:
        active = ["diamond"]
        if circle + special >= 1:
            active += ["circle", "special"]

    # ê²½ì˜ë°©ì¹¨, ìˆ«ì êµ¬ì¡° ë¬¸ì„œ
    elif num >= 3:
        active = ["number_top", "number_sub"]

    else:
        nonzero = [k for k, v in counts.items() if v > 0]
        if nonzero:
            max_cnt = max(counts[k] for k in nonzero)
            active = [k for k in nonzero if counts[k] == max_cnt]
        else:
            active = ["article"]

    return [candidate[k] for k in active]


# ===========================================================
# 3. ê¸¸ì´ ê¸°ë°˜ ìŠ¤í”Œë¦¿ í•¨ìˆ˜
# ===========================================================
def split_long_chunk_with_heading(chunk_text: str, max_chars: int) -> list[str]:
    if len(chunk_text) <= max_chars:
        return [chunk_text]

    lines = chunk_text.splitlines()
    heading = lines[0]
    body = "\n".join(lines[1:]).strip()
    paras = [p.strip() for p in body.split("\n\n") if p.strip()]

    max_body = max_chars - len(heading) - 10
    chunks = []
    buf = []

    def flush():
        nonlocal buf
        if not buf:
            return
        text = heading + "\n" + "\n\n".join(buf)
        chunks.append(text)
        buf = []

    for p in paras:
        if not buf:
            cand = p
        else:
            cand = "\n\n".join(buf + [p])

        if len(cand) <= max_body:
            buf.append(p)
        else:
            flush()
            if len(p) > max_body:
                # ê°•ì œ ìë¥´ê¸°
                s = 0
                while s < len(p):
                    part = p[s:s + max_body]
                    chunks.append(heading + "\n" + part)
                    s += max_body
            else:
                buf = [p]

    flush()
    return chunks


# ===========================================================
# 4. ê·œì •í˜• ì²­í‚¹
# ===========================================================
def split_text_by_rules(raw_text: str,
                        heading_patterns: Sequence[str],
                        max_chars: int,
                        strict_heading_only=False) -> List[str]:
    """
    strict_heading_only=True â†’ ê¸¸ì´ ê¸°ì¤€ ë¶„í•  OFF (ì¡° ë‹¨ìœ„ ìœ ì§€)
    """
    lines = raw_text.splitlines()
    compiled = [re.compile(p) for p in heading_patterns]
    coarse = []
    buf = []

    def is_heading(line):
        s = line.strip()
        for pat in compiled:
            if pat.match(s):
                return True
        return False

    def flush():
        nonlocal buf
        if not buf:
            return None
        t = "\n".join(buf).strip()
        buf = []
        return t

    for line in lines:
        if is_heading(line):
            f = flush()
            if f:
                coarse.append(f)
            buf = [line]
        else:
            buf.append(line)

    last = flush()
    if last:
        coarse.append(last)

    # ê¸¸ì´ ê¸°ë°˜ ë¶„í•  í•´ì œ (ê·œì •ë¥˜)
    if strict_heading_only:
        return [c for c in coarse if len(c.strip()) > 20]

    # structured ë¬¸ì„œ â†’ í•„ìš”ì‹œ ê¸¸ì´ ë¶„í• 
    final = []
    for ch in coarse:
        if len(ch) <= max_chars:
            final.append(ch)
        else:
            final.extend(split_long_chunk_with_heading(ch, max_chars))

    return [c for c in final if len(c.strip()) > 20]


# ===========================================================
# 5. íŒŒì¼ íƒ€ì…ë³„ chunk í•¨ìˆ˜
# ===========================================================
def extract_text_pdf(path: Path) -> str:
    import pdfplumber
    pages = []
    with pdfplumber.open(str(path)) as pdf:
        for p in pdf.pages:
            pages.append(p.extract_text() or "")
    return "\n".join(pages)


def extract_text_docx(path: Path) -> str:
    from docx import Document
    doc = Document(str(path))
    paras = [p.text for p in doc.paragraphs if p.text.strip()]
    return "\n".join(paras)


def extract_text_txt(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")


def chunk_document(path: Path) -> list[str]:
    ext = path.suffix.lower()

    # 1) í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if ext == ".pdf":
        raw = extract_text_pdf(path)
    elif ext == ".docx":
        raw = extract_text_docx(path)
    else:
        raw = extract_text_txt(path)

    raw = raw.replace("\x01", " ").replace("\u00a0", " ")

    # 2) ë¬¸ì„œ íƒ€ì… íŒë‹¨
    doc_type = detect_document_type(raw)
    print(f"   â†’ ë¬¸ì„œ íƒ€ì…: {doc_type}")

    # 3) í—¤ë” íŒ¨í„´ ìë™ ê°ì§€
    patterns = detect_heading_patterns_from_text(raw)
    print(f"   â†’ ì‚¬ìš© heading_patterns: {patterns}")

    # 4) íƒ€ì…ë³„ ì²­í‚¹ ì „ëµ
    if doc_type == "regulation":
        # ì¡° ë‹¨ìœ„ ì™„ì „ ë³´ì¡´
        return split_text_by_rules(raw, patterns, max_chars=999999, strict_heading_only=True)

    elif doc_type == "structured":
        # ì ìš©ë²”ìœ„/ì±…ì„ê³¼ê¶Œí•œ/ì •ë³´ë³´ì•ˆ ê°™ì€ ë¬¸ì„œ
        return split_text_by_rules(raw, patterns, max_chars=2000)

    else:
        # ì¼ë°˜ ë¬¸ì„œ: ë¬¸ë‹¨ + ê¸¸ì´ ê¸°ì¤€
        paras = [p.strip() for p in raw.split("\n\n") if p.strip()]
        chunks = []
        buf = []
        max_chars = 800

        for p in paras:
            candidate = "\n\n".join(buf + [p]) if buf else p
            if len(candidate) <= max_chars:
                buf.append(p)
            else:
                chunks.append("\n\n".join(buf))
                buf = [p]

        if buf:
            chunks.append("\n\n".join(buf))

        return chunks


# ===========================================================
# ë©”ì¸
# ===========================================================
def main():
    print_section("RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹ + add_chunk (PDF/DOCX/TXT í¬í•¨)")

    # ------------------------------------
    # 1) ì„œë²„ ì—°ê²°
    # ------------------------------------
    print_step(1, "ì„œë²„ ì—°ê²°")
    try:
        r = requests.get(f"{HOST_ADDRESS}/api/v1/datasets",
                         headers={"Authorization": f"Bearer {API_KEY}"})
        rag = RAGFlow(API_KEY, HOST_ADDRESS)
        print("âœ… RAGFlow ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # ------------------------------------
    # 2) dataset í´ë” ê²€ìƒ‰ (pdf + docx + txt)
    # ------------------------------------
    print_step(2, "dataset í´ë” ìŠ¤ìº”")
    dataset_dir = Path(__file__).parent / "dataset"

    pdfs = list(dataset_dir.glob("*.pdf"))
    docxs = list(dataset_dir.glob("*.docx"))
    txts = list(dataset_dir.glob("*.txt"))
    files = sorted(pdfs + docxs + txts)

    if not files:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ“‚ ì²˜ë¦¬ íŒŒì¼:")
    for f in files:
        print("   -", f.name)

    # ------------------------------------
    # 3) Dataset ìƒì„±
    # ------------------------------------
    print_step(3, "ë°ì´í„°ì…‹ ìƒì„±")
    dataset_name = f"auto_chunk_{int(time.time())}"

    parser_config = DataSet.ParserConfig(rag, {"raptor": {"use_raptor": False}})

    dataset = rag.create_dataset(
        name=dataset_name,
        description="ìë™ ì²­í‚¹ ê·œì •/ì§€ì¹¨ DOCX/PDF/TXT í¬í•¨",
        chunk_method="manual",
        embedding_model=EMBEDDING_MODEL,
        parser_config=parser_config,
    )

    print(f"âœ… Dataset ìƒì„± ì™„ë£Œ: {dataset.id}")

    # ------------------------------------
    # 4) íŒŒì¼ë³„ ì—…ë¡œë“œ + ì²­í‚¹ + add_chunk
    # ------------------------------------
    print_step(4, "íŒŒì¼ ì—…ë¡œë“œ + ì²­í‚¹")

    for fpath in files:
        print(f"\n======= {fpath.name} ì²˜ë¦¬ =======")

        with open(fpath, "rb") as f:
            blob = f.read()

        doc = dataset.upload_documents(
            [{"display_name": fpath.name, "blob": blob}]
        )[0]

        print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

        chunks = chunk_document(fpath)
        print(f"â†’ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

        for idx, c in enumerate(chunks, 1):
            doc.add_chunk(content=c)
            if idx <= 2:
                print(f"\n  [ë¯¸ë¦¬ë³´ê¸° ì²­í¬ {idx}]")
                print(c[:200] + "...")

        print(f"â†’ ì´ {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")

    # ------------------------------------
    # 5) ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # ------------------------------------
    print_step(5, "ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")

    query = "ì´ ë¬¸ì„œì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?"
    results = rag.retrieve(
        dataset_ids=[dataset.id],
        question=query,
        top_k=5,
    )

    for i, r in enumerate(results, 1):
        print(f"\n[ê²€ìƒ‰ {i}]")
        print(r.content[:200] + "...")


if __name__ == "__main__":
    main()
