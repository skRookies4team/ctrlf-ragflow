"""
RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹(Chunking) + add_chunk
HWP / PDF / PPT / DOCX / TXT ìë™ ì²˜ë¦¬ + ë¬¸ì„œ íƒ€ì… íŒë³„ + ìë™ íŒ¨í„´ ê°ì§€ ì™„ì „íŒ
"""

import os
import sys
import time
import requests
import re
import json
import pdfplumber
from pathlib import Path
from typing import List, Sequence
from dotenv import load_dotenv
from difflib import SequenceMatcher

# =======================
# 0. ê²½ë¡œ/í™˜ê²½ ì„¤ì •
# =======================
BASE_DIR = Path(__file__).resolve().parent.parent

# ragflow ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ëª¨ë“ˆ ê²½ë¡œì— ì¶”ê°€ â†’ preprocessing íŒ¨í‚¤ì§€ import ê°€ëŠ¥
sys.path.insert(0, str(BASE_DIR))

load_dotenv(BASE_DIR / ".env")

# =======================
# 1. RAGFlow SDK import
# =======================
try:
    from ragflow_sdk import RAGFlow
except ImportError:
    # sdk/python í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€ í›„ ì¬ì‹œë„
    sys.path.insert(0, str(BASE_DIR / "sdk" / "python"))
    from ragflow_sdk import RAGFlow

from ragflow_sdk.modules.dataset import DataSet

# =======================
# 2. ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ëª¨ë“ˆ import
# =======================
from preprocessing.coverters.hwp_to_docx import HwpAdapter
from preprocessing.classifier.document_classifier import DocumentClassifier
from preprocessing.pipeline import PreprocessPipeline

# =======================
# 3. í™˜ê²½ ë³€ìˆ˜
# =======================
HOST_ADDRESS = os.getenv("RAGFLOW_HOST", "http://localhost")
API_KEY = os.getenv("RAGFLOW_API_KEY")
EMBEDDING_MODEL = os.getenv(
    "RAGFLOW_EMBEDDING_MODEL",
    "text-embedding-004@Gemini"
)

if not API_KEY:
    print("âŒ RAGFLOW_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    sys.exit(1)

# =======================
# 4. ê³µìš© ê°ì²´
# =======================
hwp_adapter = HwpAdapter()
classifier = DocumentClassifier()
preprocess_pipeline = PreprocessPipeline()


# ===========================================================
# ì¶œë ¥ ìœ í‹¸
# ===========================================================
def print_section(title: str):
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_step(n: int, text: str):
    print(f"\n[ë‹¨ê³„ {n}] {text}")
    print("-" * 60)


# ===========================================================
# 1. ë¬¸ì„œ íƒ€ì… ìë™ íŒë‹¨ (í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·œì •/ë°©ì¹¨ íŒë³„ìš©)
# ===========================================================
def detect_document_type(raw_text: str) -> str:
    """
    ë¬¸ì„œ í˜•íƒœ ìë™ íŒë‹¨
    - regulation: 'ì œ nì¡°'ê°€ ë§ì´ ë“±ì¥í•˜ëŠ” ê·œì •ë¥˜
    - structured: â—‡/â—Š/â—‹/ìˆ«ì í—¤ë”ê°€ ë§ì€ ë³´ì•ˆ/ë°©ì¹¨ ë¬¸ì„œ
    - general: ì¼ë°˜ ë¬¸ì„œ (ë³´ê³ ì„œ, íšŒì˜ë¡ ë“±)
    """

    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    article = sum(1 for l in lines if re.match(r"^\s*ì œ\s*\d+\s*ì¡°", l))
    diamond = sum(1 for l in lines if re.match(r"^\s*[â—Šâ—†â—‡]", l))
    circle = sum(1 for l in lines if re.match(r"^\s*[â—‹â—â—¯O]", l))
    number = sum(1 for l in lines if re.match(r"^\s*\d+\.", l))

    paragraph_count = len([p for p in text.split("\n\n") if p.strip()])
    length = len(text)

    if article >= 3:
        return "regulation"

    if article == 0 and (diamond >= 2 or circle >= 2):
        return "structured"

    if number >= 5:
        return "structured"

    if length > 2000 and paragraph_count >= 3:
        return "general"

    return "general"


# ===========================================================
# 2. heading íŒ¨í„´ ìë™ ê°ì§€
# ===========================================================
def detect_heading_patterns_from_text(raw_text: str) -> List[str]:
    text = raw_text.replace("\x01", " ").replace("\u00a0", " ")
    lines = text.splitlines()

    candidate = {
        "article": r"^\s*ì œ\s*\d+\s*ì¡°",
        "diamond": r"^\s*[â—Šâ—†â—‡]\s*",
        "circle": r"^\s*[â—‹â—â—¯O]\s*",
        "special": r"^\s*[ä»¤â€»]\s*",
        "number_top": r"^\s*\d+\.\s+",
        "number_sub": r"^\s*\d+\.\d+\s+",
    }

    counts = {k: 0 for k in candidate.keys()}
    compiled = {k: re.compile(v) for k, v in candidate.items()}

    for l in lines:
        s = l.rstrip()
        for k, pat in compiled.items():
            if pat.match(s):
                counts[k] += 1

    article = counts["article"]
    diamond = counts["diamond"]
    circle = counts["circle"]
    special = counts["special"]
    num = counts["number_top"] + counts["number_sub"]

    # ê·œì •ë¥˜
    if article >= 3:
        active = ["article"]

    # ë³´ì•ˆê´€ë¦¬ê·œì •
    elif article == 0 and diamond >= 1:
        active = ["diamond"]
        if circle + special >= 1:
            active += ["circle", "special"]

    # ê²½ì˜ë°©ì¹¨, ìˆ«ì êµ¬ì¡° ë¬¸ì„œ
    elif num >= 3:
        active = ["number_top", "number_sub"]

    else:
        nonzero = [k for k, v in counts.items() if v > 0]
        if nonzero:
            max_cnt = max(counts[k] for k in nonzero)
            active = [k for k in nonzero if counts[k] == max_cnt]
        else:
            active = ["article"]

    return [candidate[k] for k in active]


# ===========================================================
# 3. ê¸¸ì´ ê¸°ë°˜ ìŠ¤í”Œë¦¿ í•¨ìˆ˜
# ===========================================================
def split_long_chunk_with_heading(chunk_text: str, max_chars: int) -> List[str]:
    if len(chunk_text) <= max_chars:
        return [chunk_text]

    lines = chunk_text.splitlines()
    heading = lines[0]
    body = "\n".join(lines[1:]).strip()
    paras = [p.strip() for p in body.split("\n\n") if p.strip()]

    max_body = max_chars - len(heading) - 10
    chunks: List[str] = []
    buf: List[str] = []

    def flush():
        nonlocal buf
        if not buf:
            return
        text = heading + "\n" + "\n\n".join(buf)
        chunks.append(text)
        buf = []

    for p in paras:
        if not buf:
            cand = p
        else:
            cand = "\n\n".join(buf + [p])

        if len(cand) <= max_body:
            buf.append(p)
        else:
            flush()
            if len(p) > max_body:
                # ê°•ì œ ìë¥´ê¸°
                s = 0
                while s < len(p):
                    part = p[s:s + max_body]
                    chunks.append(heading + "\n" + part)
                    s += max_body
            else:
                buf = [p]

    flush()
    return chunks


# ===========================================================
# 4. ê·œì •í˜• ì²­í‚¹
# ===========================================================
def split_text_by_rules(raw_text: str,
                        heading_patterns: Sequence[str],
                        max_chars: int,
                        strict_heading_only: bool = False) -> List[str]:
    """
    strict_heading_only=True â†’ ê¸¸ì´ ê¸°ì¤€ ë¶„í•  OFF (ì¡° ë‹¨ìœ„ ìœ ì§€)
    """
    lines = raw_text.splitlines()
    compiled = [re.compile(p) for p in heading_patterns]
    coarse: List[str] = []
    buf: List[str] = []

    def is_heading(line: str) -> bool:
        s = line.strip()
        for pat in compiled:
            if pat.match(s):
                return True
        return False

    def flush():
        nonlocal buf
        if not buf:
            return None
        t = "\n".join(buf).strip()
        buf = []
        return t

    for line in lines:
        if is_heading(line):
            f = flush()
            if f:
                coarse.append(f)
            buf = [line]
        else:
            buf.append(line)

    last = flush()
    if last:
        coarse.append(last)

    # ê¸¸ì´ ê¸°ë°˜ ë¶„í•  í•´ì œ (ê·œì •ë¥˜)
    if strict_heading_only:
        return [c for c in coarse if len(c.strip()) > 20]

    # structured ë¬¸ì„œ â†’ í•„ìš”ì‹œ ê¸¸ì´ ë¶„í• 
    final: List[str] = []
    for ch in coarse:
        if len(ch) <= max_chars:
            final.append(ch)
        else:
            final.extend(split_long_chunk_with_heading(ch, max_chars))

    return [c for c in final if len(c.strip()) > 20]


# ==========================================
# HWP / ìŠ¬ë¼ì´ë“œí˜• PDF ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë˜í¼
# ==========================================
def preprocess_to_chunks(path: Path, chunk_size: int = 1200) -> list[str]:
    """
    PreprocessPipelineì„ ì‹¤í–‰í•´ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸(list[str])ë¡œ ë³€í™˜.
    - result êµ¬ì¡°:
        {
          "result_json": {
             "num_chunks": 3,
             "chunks": [
                 {"text": "...", "meta": {...}},
                 ...
             ],
             "meta": {...}
          }
        }
    ì´ëŸ° í˜•íƒœë¥¼ ê°€ì •í•˜ê³  ì•ˆì „í•˜ê²Œ íŒŒì‹±.
    """
    result = preprocess_pipeline.run(str(path), chunk_size=chunk_size)

    # 1) resultê°€ ë¬¸ìì—´ì´ë©´ â†’ JSON íŒŒì¼ ê²½ë¡œë¼ê³  ê°€ì •í•˜ê³  ë¡œë“œ
    if isinstance(result, str):
        try:
            with open(result, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            return [result.strip()] if result.strip() else []
    else:
        data = result

    # 2) dataì—ì„œ ì‹¤ì œ chunk ëª©ë¡ êº¼ë‚´ê¸°
    items = []

    if isinstance(data, dict):
        # case 1: {"result_json": {...}}
        if "result_json" in data:
            rj = data["result_json"]
            if isinstance(rj, dict):
                # {"num_chunks": n, "chunks": [...], "meta": {...}}
                if "chunks" in rj and isinstance(rj["chunks"], list):
                    items = rj["chunks"]
            elif isinstance(rj, list):
                items = rj

        # case 2: {"chunks": [...]} í˜•íƒœ
        elif "chunks" in data and isinstance(data["chunks"], list):
            items = data["chunks"]

    elif isinstance(data, list):
        items = data

    if not items:
        return []

    # 3) ê° itemì—ì„œ í…ìŠ¤íŠ¸ë§Œ ë½‘ê¸°
    chunks: list[str] = []
    for item in items:
        if isinstance(item, dict):
            text = item.get("text") or item.get("content") or ""
        else:
            text = str(item)

        text = text.strip()
        if text:
            chunks.append(text)

    return chunks

# =========================
# CER(ë¬¸ì ì˜¤ë¥˜ìœ¨) ê³„ì‚° ìœ í‹¸ (ì„ íƒì )
# =========================
def cer(pred: str, truth: str) -> float:
    """
    CER = (ì‚½ì… + ì‚­ì œ + êµì²´) / ì •ë‹µ ê¸€ì ìˆ˜
    edit distance ê¸°ë°˜
    """
    import numpy as np

    p = list(pred)
    t = list(truth)

    dp = np.zeros((len(t) + 1, len(p) + 1), dtype=int)

    for i in range(len(t) + 1):
        dp[i][0] = i
    for j in range(len(p) + 1):
        dp[0][j] = j

    for i in range(1, len(t) + 1):
        for j in range(1, len(p) + 1):
            cost = 0 if t[i - 1] == p[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,      # ì‚­ì œ
                dp[i][j - 1] + 1,      # ì‚½ì…
                dp[i - 1][j - 1] + cost  # êµì²´
            )

    return dp[len(t)][len(p)] / max(1, len(t))


def eval_cer_for_pdf_text(pdf_path: Path, extracted_text: str) -> None:
    """
    ì›ë³¸ PDFì— ëŒ€ì‘í•˜ëŠ” ì •ë‹µ í…ìŠ¤íŠ¸(.txt)ë¥¼ ì½ì–´ì„œ CER ì¶œë ¥.

    ì •ë‹µ íŒŒì¼ ìœ„ì¹˜:
        sample/dataset/solution/<pdfíŒŒì¼ëª…>.txt
        ì˜ˆ) ì´ì‚¬íšŒê·œì •.pdf â†’ solution/ì´ì‚¬íšŒê·œì •.txt
    """

    gt_root = pdf_path.parent / "solution"
    gt_path = gt_root / f"{pdf_path.stem}.txt"

    if not gt_path.exists():
        print(f"   âš  CER ìŠ¤í‚µ: ì •ë‹µ íŒŒì¼ ì—†ìŒ â†’ {gt_path}")
        return

    truth = gt_path.read_text(encoding="utf-8", errors="ignore")
    pred = extracted_text

    truth_norm = truth.replace("\r\n", "\n").strip()
    pred_norm = pred.replace("\r\n", "\n").strip()

    score = cer(pred_norm, truth_norm)
    print(f"   âœ… CER í‰ê°€ ê²°ê³¼: {score * 100:.2f}% (ë¬¸ì ì˜¤ë¥˜ìœ¨)")
    print(f"      â†’ ë¬¸ì ì •í™•ë„(ëŒ€ëµ): {(1 - score) * 100:.2f}%")


# ===========================================================
# 5. DOCX / TXT ì „ìš© chunk í•¨ìˆ˜
#    (PDF/HWP/PPTëŠ” ìƒìœ„ ë£¨í”„ì—ì„œ ì²˜ë¦¬)
# ===========================================================
def extract_text_docx(path: Path) -> str:
    from docx import Document
    doc = Document(str(path))
    paras = [p.text for p in doc.paragraphs if p.text.strip()]
    return "\n".join(paras)


def extract_text_txt(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")


def chunk_document(path: Path) -> List[str]:
    """
    DOCX / TXT ì „ìš© ì²­í‚¹
    (PDF/HWP/HWPX/PPT ëŠ” ìƒìœ„ for ë£¨í”„ì—ì„œ ë³„ë„ ì²˜ë¦¬)
    """
    ext = path.suffix.lower()

    if ext == ".docx":
        raw = extract_text_docx(path)
    else:
        raw = extract_text_txt(path)

    # ë¬¸ì„œ íƒ€ì… ë¶„ì„ í›„ ê¸°ì¡´ ê·œì • ì²­í‚¹
    doc_type = detect_document_type(raw)
    patterns = detect_heading_patterns_from_text(raw)

    if doc_type == "regulation":
        return split_text_by_rules(raw, patterns, max_chars=999999, strict_heading_only=True)
    elif doc_type == "structured":
        return split_text_by_rules(raw, patterns, max_chars=2000)
    else:
        # ì¼ë°˜ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼
        paras = [p.strip() for p in raw.split("\n\n") if p.strip()]
        chunks: List[str] = []
        buf: List[str] = []

        for p in paras:
            candidate = "\n\n".join(buf + [p]) if buf else p
            if len(candidate) <= 800:
                buf.append(p)
            else:
                if buf:
                    chunks.append("\n\n".join(buf))
                buf = [p]
        if buf:
            chunks.append("\n\n".join(buf))
        return chunks


def chunk_text_pdf(path: Path) -> list[str]:
    """
    í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF â†’ pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„
    DOCXë‘ ë˜‘ê°™ì€ ê·œì •/êµ¬ì¡° ë¬¸ì„œ ì²­í‚¹ ë¡œì§ ì ìš©
    """
    pages = []
    with pdfplumber.open(str(path)) as pdf:
        for p in pdf.pages:
            pages.append(p.extract_text() or "")

    raw = "\n".join(pages)

    # ë¬¸ì„œ íƒ€ì… ë¶„ì„
    doc_type = detect_document_type(raw)
    patterns = detect_heading_patterns_from_text(raw)

    if doc_type == "regulation":
        # ì œ 1 ì¡°, ì œ 2 ì¡° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê¸¸ì´ëŠ” ì›¬ë§Œí•˜ë©´ ì•ˆ ìë¦„
        return split_text_by_rules(raw, patterns, max_chars=999999, strict_heading_only=True)
    elif doc_type == "structured":
        # ìˆ«ì í—¤ë”/ëª©ì°¨ê°€ ë§ì€ ê²½ìš°: ì¡°ê¸ˆ ë” ì˜ê²Œ
        return split_text_by_rules(raw, patterns, max_chars=2000)
    else:
        # ì¼ë°˜ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ â†’ DOCXë‘ ë™ì¼í•œ ë‹¨ë½ ê¸°ë°˜ ì²­í‚¹
        paras = [p.strip() for p in raw.split("\n\n") if p.strip()]
        chunks = []
        buf = []

        for p in paras:
            candidate = "\n\n".join(buf + [p]) if buf else p
            if len(candidate) <= 800:
                buf.append(p)
            else:
                if buf:
                    chunks.append("\n\n".join(buf))
                buf = [p]
        if buf:
            chunks.append("\n\n".join(buf))

        return chunks


# ===========================================================
# ë©”ì¸
# ===========================================================
MAX_CHUNK_LEN = 8000  # ë„ˆë¬´ ê¸´ ì²­í¬ ë°©ì§€ìš© (í•„ìš”í•˜ë©´ 4000~6000 ì •ë„ë¡œ ì¤„ì—¬ë„ ë¨)

def compare_with_solution(dataset_dir: Path, fpath: Path, chunks: list[str]):
    solution_dir = dataset_dir / "solution"
    solution_txt_path = solution_dir / f"{fpath.stem}.txt"

    if not solution_txt_path.exists():
        print(f"  [ìœ ì‚¬ë„] solution txt ì—†ìŒ â†’ {solution_txt_path.name} (ìŠ¤í‚µ)")
        return

    try:
        solution_text = solution_txt_path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        solution_text = solution_txt_path.read_text(encoding="cp949")

    chunk_text = "\n".join(c for c in chunks if c and c.strip())

    sim = SequenceMatcher(None, chunk_text, solution_text).ratio()

    print(f"  [ìœ ì‚¬ë„] ì „ì²´ ì²­í‚¹ ê²°ê³¼ vs solution/{solution_txt_path.name}: {sim*100:.2f}%")


# ===========================================================
# ì²­í¬ ì¶”ê°€ ìœ í‹¸ (ì‹¬í”Œ ë²„ì „)
# ===========================================================
def add_chunks_safe(doc, chunks):
    """
    ì²­í¬ë“¤ì„ RAGFlow docì— ì¶”ê°€.
    - ì˜ˆì „ì²˜ëŸ¼: ìƒì„±ëœ ì²­í¬ ìˆ˜ + ë¯¸ë¦¬ë³´ê¸° 1,2ë²ˆë§Œ ì¶œë ¥
    """
    print(f"â†’ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

    for idx, c in enumerate(chunks, start=1):
        if not c or not c.strip():
            continue

        # í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ê¸¸ì´ ì²´í¬í•´ì„œ ì˜ë¼ ë„£ì„ ìˆ˜ë„ ìˆì§€ë§Œ
        # ì§€ê¸ˆì€ ë‹¤ ì§§ìœ¼ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ ì¶”ê°€
        doc.add_chunk(content=c)

        # ë¯¸ë¦¬ë³´ê¸°ëŠ” ì•ì˜ ë‘ ê°œë§Œ
        if idx <= 2:
            print(f"\n  [ë¯¸ë¦¬ë³´ê¸° ì²­í¬ {idx}]")
            print(c[:200] + "...")

    print(f"â†’ ì´ {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")



# ===========================================================
# ë©”ì¸
# ===========================================================
def main():
    print_section("RAGFlow ì»¤ìŠ¤í…€ ì²­í‚¹ + add_chunk (HWP/PDF/PPT/DOCX/TXT í¬í•¨)")

    # ------------------------------------
    # 1) ì„œë²„ ì—°ê²°
    # ------------------------------------
    print_step(1, "ì„œë²„ ì—°ê²°")
    try:
        _ = requests.get(
            f"{HOST_ADDRESS}/api/v1/datasets",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=5,
        )
        rag = RAGFlow(API_KEY, HOST_ADDRESS)
        print("âœ… RAGFlow ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # ------------------------------------
    # 2) dataset í´ë” ê²€ìƒ‰
    # ------------------------------------
    print_step(2, "dataset í´ë” ìŠ¤ìº”")
    dataset_dir = Path(__file__).parent / "dataset"

    pdfs = list(dataset_dir.glob("*.pdf"))
    ppts = list(dataset_dir.glob("*.ppt")) + list(dataset_dir.glob("*.pptx"))
    hwps = list(dataset_dir.glob("*.hwp")) + list(dataset_dir.glob("*.hwpx"))
    docxs = list(dataset_dir.glob("*.docx"))
    txts = list(dataset_dir.glob("*.txt"))

    files = sorted(pdfs + ppts + hwps + docxs + txts)

    if not files:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ“‚ ì²˜ë¦¬ íŒŒì¼:")
    for f in files:
        print("   -", f.name)

    # ------------------------------------
    # 3) Dataset ìƒì„±
    # ------------------------------------
    print_step(3, "ë°ì´í„°ì…‹ ìƒì„±")
    dataset_name = f"auto_chunk_{int(time.time())}"

    parser_config = DataSet.ParserConfig(rag, {"raptor": {"use_raptor": False}})

    dataset = rag.create_dataset(
        name=dataset_name,
        description="ìë™ ì²­í‚¹ (HWP/ìŠ¬ë¼ì´ë“œ/í…ìŠ¤íŠ¸ PDF/DOCX/TXT í˜¼í•©)",
        chunk_method="manual",
        embedding_model=EMBEDDING_MODEL,
        parser_config=parser_config,
    )

    print(f"âœ… Dataset ìƒì„± ì™„ë£Œ: {dataset.id}")

    # ------------------------------------
    # 4) íŒŒì¼ë³„ ì—…ë¡œë“œ + ì²­í‚¹
    # ------------------------------------
    print_step(4, "íŒŒì¼ ì—…ë¡œë“œ + ì²­í‚¹")

    for fpath in files:
        fpath = fpath.resolve()
        ext = fpath.suffix.lower().lstrip(".")
        print(f"\n======= {fpath.name} ì²˜ë¦¬ =======")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4-1. HWP/HWPX â†’ DOCXë¡œ ë³€í™˜
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ext in ("hwp", "hwpx"):
            print(f"[HWP] {fpath.name} â†’ DOCXë¡œ ë³€í™˜")
            docx_path = hwp_adapter.to_docx(str(fpath))
            fpath = Path(docx_path)
            ext = "docx"

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4-2. PDF / PPT / PPTX ì²˜ë¦¬
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ext in ("pdf", "ppt", "pptx"):
            if ext == "pdf":
                doc_type = classifier.classify(str(fpath))
            else:
                doc_type = "ppt"

            print(f"â†’ ë¬¸ì„œ íƒ€ì…: {doc_type}")

            # 1-1) í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF â†’ ìš°ë¦¬ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©
            if doc_type == "text_pdf":
                print("â†’ [í…ìŠ¤íŠ¸ PDF] ë¡œì»¬ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©")

                with open(fpath, "rb") as fb:
                    blob = fb.read()

                doc = dataset.upload_documents(
                    [{"display_name": fpath.name, "blob": blob}]
                )[0]
                print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

                chunks = chunk_text_pdf(fpath)

                # â˜… PDFë„ solution txtì™€ ìœ ì‚¬ë„ í™•ì¸
                compare_with_solution(dataset_dir, fpath, chunks)

                add_chunks_safe(doc, chunks)
                continue
            # 1-2) ì´ë¯¸ì§€ ê¸°ë°˜ PDF / PPT â†’ PreprocessPipeline + add_chunk
            print("â†’ [ì´ë¯¸ì§€/ìŠ¬ë¼ì´ë“œ] PreprocessPipeline + add_chunk ì‚¬ìš©")

            print("â†’ [ì´ë¯¸ì§€/ìŠ¬ë¼ì´ë“œ] PreprocessPipeline + add_chunk ì‚¬ìš©")

            # ğŸŸ¢ 1) ë¨¼ì € RAGFlowì— ë¬¸ì„œ ì—…ë¡œë“œí•´ì„œ doc ìƒì„±
            with open(fpath, "rb") as fb:
                blob = fb.read()

            doc = dataset.upload_documents(
                [{"display_name": fpath.name, "blob": blob}]
            )[0]
            print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

            # ğŸŸ¢ 2) PreprocessPipeline ì‹¤í–‰
            pipeline_result = preprocess_pipeline.run(
                str(fpath),      # input_pdf
                chunk_size=1200, # í•„ìš”í•˜ë©´ ì¡°ì ˆ
            )

            print("â†’ PreprocessPipeline ì™„ë£Œ")

            # ğŸŸ¢ 3) íŒŒì´í”„ë¼ì¸ì—ì„œ ë‚˜ì˜¨ ì²­í¬ ë½‘ê¸°
            chunks = [c["text"] for c in pipeline_result["result_json"]["chunks"]]
            print(f"â†’ íŒŒì´í”„ë¼ì¸ ì²­í¬ {len(chunks)}ê°œ ë°˜í™˜")

            # ğŸŸ¢ 4) RAGFlow docì— add_chunk
            for idx, c in enumerate(chunks, 1):
                doc.add_chunk(content=c)
                if idx <= 2:
                    print(f"\n  [ë¯¸ë¦¬ë³´ê¸° ì²­í¬ {idx}]")
                    print(c[:200] + ("..." if len(c) > 200 else ""))

            print(f"â†’ ì´ {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")

            # ì´ íŒŒì¼ì€ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëë‚¬ìœ¼ë‹ˆê¹Œ ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°
            continue

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4-3. DOCX / TXT â†’ ê¸°ì¡´ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("â†’ [DOCX/TXT] ê¸°ì¡´ ê·œì •í˜• ì²­í‚¹ ì‚¬ìš©")

        with open(fpath, "rb") as fb:
            blob = fb.read()

        doc = dataset.upload_documents(
            [{"display_name": fpath.name, "blob": blob}]
        )[0]
        print(f"â†’ ì—…ë¡œë“œ ì™„ë£Œ (doc.id={doc.id})")

        chunks = chunk_document(fpath)
        compare_with_solution(dataset_dir, fpath, chunks)
        add_chunks_safe(doc, chunks)

    # ------------------------------------
    # 5) ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # ------------------------------------
    print_step(5, "ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")

    query = "ì´ ë¬¸ì„œì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?"
    results = rag.retrieve(
        dataset_ids=[dataset.id],
        question=query,
        top_k=5,
    )

    for i, r in enumerate(results, 1):
        print(f"\n[ê²€ìƒ‰ {i}]")
        print(r.content[:200] + "...")


if __name__ == "__main__":
    main()
